{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import time\n",
    "from jax_smi import initialise_tracking\n",
    "initialise_tracking()\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(gpt2_model_type='gpt2', seed=556, out_dir='out', shuffle_buffer_size=128, eval_interval=500, eval_steps=16, eval_only=False, keep_checkpoints=3, batch_size=128, train_steps=30, weight_decay=0.0, grad_clip=10.0, gradient_accumulation_steps=1, betas=(0.9, 0.95), learning_rate=StaticLRConfig(init_value=0.0001), wandb=WandbConfig(entity='ars22', project='star_graph', name='gpt2', mode='online', notes=''), model=GPTConfig(block_size=1024, vocab_size=50257, num_layers=12, num_heads=12, num_embeds=768, dropout_rate=0.1, use_bias=True, dtype=None), remat=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Tuple, Optional, Union\n",
    "from EasyLM.models.gpt2.gpt2_model import GPT, GPTConfig, get_pretrained_params\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class WandbConfig:\n",
    "    \"\"\"\n",
    "    wandb logging configuration\n",
    "    \"\"\"\n",
    "    entity: str = 'ars22'\n",
    "    \"\"\"username or team name where you're sending runs\"\"\"\n",
    "    project: str = 'star_graph'\n",
    "    \"\"\"project name\"\"\"\n",
    "    name: str = 'gpt2'\n",
    "    \"\"\"experiment name\"\"\"\n",
    "    mode: str = 'online'\n",
    "    \"\"\"'offline', 'online', or 'disabled'\"\"\"\n",
    "    notes: str = ''\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CosineDecayScheduleConfig:\n",
    "    init_value: float = 0.0\n",
    "    peak_value: float = 2.5e-4\n",
    "    warmup_steps: int = 2000\n",
    "    decay_steps: int = 150000\n",
    "    end_value: float = 1e-5\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StaticLRConfig:\n",
    "    init_value: float = 1e-4\n",
    "\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class TrainConfig:\n",
    "    gpt2_model_type: str = 'gpt2' # gpt2 model type\n",
    "    seed: int = 556\n",
    "    out_dir: str = 'out'                        # output directory for checkpoints (can be gcs path)\n",
    "    shuffle_buffer_size: int = 128\n",
    "    eval_interval: int = 500\n",
    "    eval_steps: int = 16        # evaluate for this number of steps (per-device)\n",
    "    eval_only: bool = False     # if True, script exits right after the first eval\n",
    "    keep_checkpoints: int = 3   # number of historical checkpoints to keep\n",
    "    batch_size: int = 128        # per-device batch size\n",
    "    train_steps: int = 30     # total number of training iterations\n",
    "    weight_decay: float = 0.0  # not applied to bias and embedding parameters\n",
    "    grad_clip: float = 10.0      # gradient norm clipping magnitude\n",
    "    gradient_accumulation_steps: int = 1    # used to simulate larger batch sizes\n",
    "    betas: Tuple[float, float] = (0.9, 0.95) # adamw optimizer betas\n",
    "    # learning_rate: CosineDecayScheduleConfig = field(default_factory=CosineDecayScheduleConfig)\n",
    "    learning_rate: StaticLRConfig = field(default_factory=StaticLRConfig)\n",
    "    wandb: WandbConfig = field(default_factory=WandbConfig) # wandb logging\n",
    "    model: GPTConfig = field(default_factory=GPTConfig)     # gpt model config\n",
    "    remat: bool = False    # set to True to rematerialize gradients during backward pass\n",
    "\n",
    "\n",
    "def get_default_config() -> TrainConfig:\n",
    "    return TrainConfig()\n",
    "\n",
    "config = get_default_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax.core import FrozenDict, frozen_dict\n",
    "from flax.training import checkpoints\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "def prefix_target_list(filename=None):\n",
    "    \"\"\"\n",
    "    Load graphs and split them into prefix and target and return the list\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        prefix = line.strip().split('=')[0] + '='\n",
    "        target = line.strip().split('=')[1]\n",
    "        # target = target.split(',')[1]\n",
    "        data_list.append((prefix, target))\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class Graphs(Dataset):\n",
    "    def __init__(self, tokenizer, n_samples, data_path):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_samples = n_samples\n",
    "        self.data_path = data_path\n",
    "        self.eval_mode = False\n",
    "        self.data_file = prefix_target_list(self.data_path)\n",
    "        self.tokenized, self.num_prefix_tokens, self.num_target_tokens = self.tokenize(self.data_file[:n_samples])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.eval_mode:\n",
    "            # In eval mode return the entire sequence\n",
    "            return self.tokenized[idx].to(self.device)\n",
    "\n",
    "        # Create inputs\n",
    "        x = self.tokenized[idx].clone()\n",
    "        y = torch.cat([-torch.ones((self.num_prefix_tokens - 1, )),\n",
    "                       x[self.num_prefix_tokens:].clone()])\n",
    "        return x[:-1], y.long()\n",
    "\n",
    "    def tokenize(self, data_list):\n",
    "        \"\"\"\n",
    "        Takes a list of prefix-target pairs, tokenizes and concatenates them\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        prefix_len = len(self.tokenizer.encode(data_list[0][0]))\n",
    "        target_len = len(self.tokenizer.encode(data_list[0][1]))\n",
    "        same_len = True\n",
    "\n",
    "        for prefix, target in data_list:\n",
    "            prefix = torch.tensor(self.tokenizer.encode(prefix))\n",
    "            target = torch.tensor(self.tokenizer.encode(target))\n",
    "            if not (len(prefix) == prefix_len and len(target) == target_len):\n",
    "                same_len = False\n",
    "            seq = torch.concatenate([prefix, target], dim=-1).long()\n",
    "            out.append(seq)\n",
    "\n",
    "        # Check if all prefixes and all targets have the same length\n",
    "        if not same_len:\n",
    "            print('Not all prefixes or targets have the same length!!')\n",
    "        else:\n",
    "            print('Equal sequence lengths!')\n",
    "\n",
    "        return out, prefix_len, target_len\n",
    "\n",
    "    def eval(self):\n",
    "        # Switch to \"eval\" mode when generating sequences without teacher-forcing\n",
    "        self.eval_mode = True\n",
    "\n",
    "    def train(self):\n",
    "        # Switch back to \"train\" mode for teacher-forcing\n",
    "        self.eval_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "(tensor([21, 11, 24, 91, 22, 11, 21, 91, 24, 11, 15, 91, 20, 11, 17, 91, 22, 11,\n",
      "        19, 91, 19, 11, 20, 14, 22, 11, 17, 28, 22, 11, 19, 11, 20, 11]), tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 11, 19, 11, 20, 11, 17])) 6,9|7,6|9,0|5,2|7,4|4,5/7,2=7,4,5, 7,4,5,2\n"
     ]
    }
   ],
   "source": [
    "# LOAD TOKENIZER\n",
    "from transformers import AutoTokenizer # type: ignore\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.gpt2_model_type)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# LOAD DATASET\n",
    "data_path = 'deg_2_path_4_nodes_10'\n",
    "train_path, test_path = data_path + '_train_200000.txt', data_path + '_test_20000.txt'\n",
    "train_data = Graphs(tokenizer=tokenizer, n_samples=20000, data_path=train_path)\n",
    "test_data = Graphs(tokenizer=tokenizer, n_samples=500, data_path=test_path)\n",
    "train_data.train()\n",
    "\n",
    "# sanity check\n",
    "print(train_data[0], tokenizer.decode(train_data[0][0]), tokenizer.decode(train_data[0][1][-train_data.num_target_tokens:]))\n",
    "\n",
    "# LOAD DATALOADER\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, drop_last=True) \n",
    "test_loader = DataLoader(test_data, batch_size=config.batch_size, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer vocab size:  50257 28 7\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer vocab size: \", tokenizer.vocab_size, train_data.num_prefix_tokens, train_data.num_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_decay_mask(params: FrozenDict) -> FrozenDict:\n",
    "    \"\"\" pytree mask for non-bias parameters \"\"\"\n",
    "    flat_params = flax.traverse_util.flatten_dict(params)\n",
    "    flat_param_mask = {k: k[-1] not in ('bias', 'embedding', 'scale') for k in flat_params.keys()}\n",
    "    param_mask = flax.traverse_util.unflatten_dict(flat_param_mask)\n",
    "    return frozen_dict.freeze(param_mask)\n",
    "\n",
    "def init_train_state(key, config: TrainConfig, learning_rate) -> TrainState:\n",
    "\n",
    "    if config.remat:\n",
    "        model = flax.linen.remat(GPT,\n",
    "            static_argnums=(2,),\n",
    "            policy=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims)(config.model)\n",
    "    else:\n",
    "        config.model, params = get_pretrained_params(config.gpt2_model_type)\n",
    "        model = GPT(config.model)    \n",
    "        model.init(key)\n",
    "\n",
    "    optimizer = optax.chain(\n",
    "        # Apply weight decay only to non-bias parameters\n",
    "        optax.clip_by_global_norm(config.grad_clip),\n",
    "        optax.adamw(learning_rate, *config.betas, weight_decay=config.weight_decay, mask=param_decay_mask(params)),\n",
    "        optax.apply_every(config.gradient_accumulation_steps),\n",
    "    )\n",
    "\n",
    "    train_state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer)\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def count_params(params: FrozenDict) -> int:\n",
    "    p = jax.tree_util.tree_map(lambda a: a.size if isinstance(a, jnp.ndarray) else 0, params)\n",
    "    return jax.tree_util.tree_reduce(lambda a, b: a + b, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====  init parameters ============\n",
    "key = jax.random.PRNGKey(config.seed)\n",
    "key, key_params, key_dropout, key_generation = jax.random.split(key, 4)\n",
    "# make sure dropout keys are different for each device (local and global)\n",
    "key_dropout = jax.random.fold_in(key_dropout, jax.process_index())\n",
    "keys_dropout = jax.random.split(key_dropout, jax.local_device_count())\n",
    "key_gen = jax.random.split(key_generation, jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "source": [
    "learning_rate = config.learning_rate.init_value\n",
    "train_state = init_train_state(key_params, config, learning_rate)\n",
    "num_params = count_params(train_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters: {num_params:,}\") # 774,030,080 for gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict, freeze, unfreeze\n",
    "from transformers import FlaxGPT2LMHeadModel\n",
    "hf_model = FlaxGPT2LMHeadModel.from_pretrained(config.gpt2_model_type)\n",
    "hf_params = hf_model.init_weights(key_params, (2, config.model.block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicate model\n",
    "train_state = replicate(train_state)\n",
    "hf_params = replicate(hf_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_and_accuracy(logits, tokens, valid=None):\n",
    "    if valid is None:\n",
    "        valid = jnp.ones(tokens.shape[:2])\n",
    "    valid = valid.astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "    token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(logits, axis=-1),\n",
    "            jnp.expand_dims(tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n",
    "    loss = -(jnp.sum(token_log_prob) / jnp.sum(valid))\n",
    "    # old: loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n",
    "    # changed to match hf implementation\n",
    "    correct = jnp.where(\n",
    "        valid > 0.0,\n",
    "        jnp.argmax(logits, axis=-1) == tokens,\n",
    "        jnp.array(False)\n",
    "    )\n",
    "    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0))\n",
    "def train_step(state: TrainState, input_tokens: jnp.ndarray, target_tokens: jnp.ndarray, dropout_key):\n",
    "    dropout_key = jax.random.fold_in(dropout_key, state.step)\n",
    "    def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
    "        logits = state.apply_fn(params, input_tokens, False, rngs={'dropout': dropout_key})\n",
    "        \n",
    "        logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "        token_log_prob = jnp.squeeze(\n",
    "            jnp.take_along_axis(\n",
    "                jax.nn.log_softmax(logits, axis=-1),\n",
    "                jnp.expand_dims(target_tokens, -1),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        prob_hard_token = jnp.exp(token_log_prob[:, train_data.num_prefix_tokens+1]).mean()\n",
    "        \n",
    "        loss, acc = cross_entropy_loss_and_accuracy(\n",
    "            logits, target_tokens, (target_tokens > 0).astype(jnp.int32))\n",
    "        \n",
    "        return loss, (prob_hard_token, acc)\n",
    "    # per-device loss and grads\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (prob_hard_token, acc)), grads = grad_fn(state.params)\n",
    "    # average gradients across devices\n",
    "    prob_hard_token = jax.lax.pmean(prob_hard_token, axis_name=\"batch\")\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
    "    loss = jax.lax.pmean(loss, axis_name=\"batch\")\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    return loss, acc, prob_hard_token, new_state\n",
    "\n",
    "\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "def convert_jax_params_to_hf(hf_params, jax_params) -> FrozenDict:\n",
    "    hf_params = unfreeze(hf_params)\n",
    "    \n",
    "    for k in ['ln_f', 'wpe', 'wte']:\n",
    "        hf_params['transformer'][k] = jax_params[k]\n",
    "    for k in hf_params['transformer']['h'].keys():\n",
    "        hf_params['transformer']['h'][k] = jax_params[k] \n",
    "\n",
    "    hf_params = flatten_dict(hf_params, sep='.')\n",
    "    for k in hf_params.keys():\n",
    "        if k.endswith('attn.c_attn.kernel'):\n",
    "            hf_params[k] = hf_params[k].T\n",
    "        elif k.endswith('attn.c_proj.kernel'):\n",
    "            hf_params[k] = hf_params[k].T\n",
    "        elif len(k.split('.')) > 3 and k.split('.')[3] == 'mlp' and k.endswith('kernel'):\n",
    "            hf_params[k] = hf_params[k].T\n",
    "    hf_params = unflatten_dict({k: v for k, v in hf_params.items()}, sep='.')\n",
    "    return freeze(hf_params)\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0))\n",
    "def eval_step(hf_params, state, input_tokens: jnp.ndarray, target_tokens: jnp.ndarray):\n",
    "    hf_params = convert_jax_params_to_hf(hf_params, state.params['params'])\n",
    "    output = hf_model.generate(\n",
    "        input_tokens[:, :train_data.num_prefix_tokens],\n",
    "        params=hf_params,\n",
    "        max_new_tokens=train_data.num_target_tokens,\n",
    "        min_length=train_data.num_target_tokens+train_data.num_prefix_tokens, \n",
    "        do_sample=False, \n",
    "        attention_mask=jnp.ones_like(input_tokens[:, :train_data.num_prefix_tokens]))\n",
    "    acc = ((output[0][:, -train_data.num_target_tokens:] == target_tokens[:, -train_data.num_target_tokens:]).sum(1) == train_data.num_target_tokens).mean()\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    return acc\n",
    "\n",
    "max_new_tokens = train_data.num_target_tokens\n",
    "num_beams=5 \n",
    "num_return_sequences=5\n",
    "temperature=1.0\n",
    "\n",
    "from flax.core import FrozenDict, freeze, unfreeze\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0))\n",
    "def generate_negative_data(hf_params, train_state, input_tokens, key):\n",
    "    hf_params = convert_jax_params_to_hf(hf_params, train_state.params['params'])\n",
    "    return hf_model.generate(\n",
    "        input_tokens[:, :train_data.num_prefix_tokens],\n",
    "        params=hf_params,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        min_length=train_data.num_target_tokens+train_data.num_prefix_tokens,\n",
    "        prng_key=key, \n",
    "        num_beams=5, \n",
    "        num_return_sequences=5, \n",
    "        temperature=1.0,\n",
    "        attention_mask=jnp.ones_like(input_tokens[:, :train_data.num_prefix_tokens]))\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(hf_params, state: TrainState, loader: DataLoader) -> jnp.ndarray:\n",
    "    accs = []\n",
    "    for batch in loader:\n",
    "        input_tokens, target_tokens = batch\n",
    "        input_tokens = jnp.array(input_tokens)\n",
    "        target_tokens = jnp.array(target_tokens)\n",
    "        input_tokens = input_tokens.reshape(jax.local_device_count(), -1, input_tokens.shape[-1])\n",
    "        target_tokens = target_tokens.reshape(jax.local_device_count(), -1, target_tokens.shape[-1])\n",
    "        acc = eval_step(hf_params, state, input_tokens, target_tokens)\n",
    "        accs.append(acc)\n",
    "    return jnp.mean(jnp.stack(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.val = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val += val * num\n",
    "        self.num += num\n",
    "\n",
    "    def get(self, percentage=False):\n",
    "        val = self.val / self.num * 100 if percentage else self.val / self.num\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/1000 [00:00<?, ?it/s]/home/asetlur/.local/lib/python3.8/site-packages/jax/_src/interpreters/mlir.py:246: RuntimeWarning: overflow encountered in cast\n",
      "  x = np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n",
      "train loss: 1.9982390403747559 phard: 0.10577373206615448 forcing train acc: 35.286460876464844 policy train acc: 0.0 policy test acc: 0.0:   0%|          | 0/1000 [01:28<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "train loss: 0.000702388642821461 phard: 0.7955995798110962 forcing train acc: 99.98456573486328 policy train acc: 99.9949951171875 policy test acc: 100.0: : 6329it [07:18, 21.39it/s]                                    "
     ]
    }
   ],
   "source": [
    "# with jax.disable_jit():\n",
    "train_iter = iter(train_loader)\n",
    "pbar = tqdm(range(8000), total=1000, desc='training')\n",
    "train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter() \n",
    "policy_train_acc, policy_test_acc = 0., 0.\n",
    "for step in pbar:\n",
    "    try:\n",
    "        input_tokens, target_tokens = next(train_iter)\n",
    "        target_tokens[:, train_data.num_prefix_tokens+2] = -1\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "    input_tokens = jnp.array(input_tokens)\n",
    "    target_tokens = jnp.array(target_tokens) \n",
    "    input_tokens = input_tokens.reshape(jax.device_count(), -1, input_tokens.shape[-1])\n",
    "    target_tokens = target_tokens.reshape(jax.device_count(), -1, target_tokens.shape[-1])\n",
    "    loss, acc, phard_token, train_state = train_step(train_state, input_tokens, target_tokens, keys_dropout)\n",
    "    train_loss.update(loss.mean(), input_tokens.shape[1] * jax.device_count())  \n",
    "    train_acc.update(acc.mean(), input_tokens.shape[1] * jax.device_count())    \n",
    "    phard.update(phard_token.mean(), input_tokens.shape[1] * jax.device_count())\n",
    "    if step % 10 == 0:\n",
    "        pbar.set_description(f'train loss: {train_loss.get()} phard: {phard.get()} forcing train acc: {train_acc.get(percentage=True)} policy train acc: {100. * policy_train_acc} policy test acc: {100. * policy_test_acc}')\n",
    "    if step % config.eval_interval == 0:\n",
    "        policy_train_acc = evaluate(hf_params, train_state, train_loader)\n",
    "        policy_test_acc = evaluate(hf_params, train_state, test_loader)\n",
    "        train_loss, train_acc = AverageMeter(), AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 16, 11, 24, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 23, 11, 22, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 19, 11, 18, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 19, 11, 21, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 15, 11, 22, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 11, 20, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 11, 15, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 21, 11, 22, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 16, 11, 22, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 24, 11, 18, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 16, 11, 20, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 23, 11, 24, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 21, 11, 20, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 19, 11, 20, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 15, 11, 22, -1, -1,\n",
       "        -1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 24, 11, 17, -1, -1,\n",
       "        -1, -1]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',1,5,6',\n",
       " ',9,6,1',\n",
       " ',4,3,5',\n",
       " ',8,5,9',\n",
       " ',1,7,2',\n",
       " ',6,4,5',\n",
       " ',1,7,4',\n",
       " ',0,8,7',\n",
       " ',0,6,1',\n",
       " ',1,8,2',\n",
       " ',8,5,6',\n",
       " ',2,1,5',\n",
       " ',5,6,0',\n",
       " ',5,7,8',\n",
       " ',1,3,9',\n",
       " ',6,2,9',\n",
       " ',7,1,6',\n",
       " ',7,5,0',\n",
       " ',8,7,9',\n",
       " ',1,8,6',\n",
       " ',7,8,2',\n",
       " ',1,0,2',\n",
       " ',9,8,3',\n",
       " ',0,1,8',\n",
       " ',6,5,7',\n",
       " ',1,8,7',\n",
       " ',3,0,5',\n",
       " ',6,7,9',\n",
       " ',3,9,0',\n",
       " ',6,3,2',\n",
       " ',6,1,0',\n",
       " ',3,7,8',\n",
       " ',0,8,1',\n",
       " ',1,7,2',\n",
       " ',4,0,3',\n",
       " ',4,5,7',\n",
       " ',4,5,6',\n",
       " ',3,4,6',\n",
       " ',5,2,4',\n",
       " ',7,5,8',\n",
       " ',6,2,0',\n",
       " ',0,3,7',\n",
       " ',6,2,1',\n",
       " ',9,5,4',\n",
       " ',9,3,2',\n",
       " ',1,7,8',\n",
       " ',6,1,3',\n",
       " ',1,0,9',\n",
       " ',7,3,2',\n",
       " ',2,8,5',\n",
       " ',1,5,7',\n",
       " ',5,9,1',\n",
       " ',0,8,1',\n",
       " ',7,8,0',\n",
       " ',7,0,4',\n",
       " ',5,8,0',\n",
       " ',4,3,8',\n",
       " ',1,4,5',\n",
       " ',3,5,7',\n",
       " ',1,6,9',\n",
       " ',4,0,6',\n",
       " ',3,0,2',\n",
       " ',8,5,6',\n",
       " ',8,4,1']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_tokens, target_tokens = next(train_iter)\n",
    "tokenizer.batch_decode(target_tokens.reshape(-1, target_tokens.shape[-1])[:, train_data.num_prefix_tokens:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(state, input_tokens, target_tokens, dropout_key):\n",
    "    logits = state.apply_fn(state.params, input_tokens, False, rngs={'dropout': dropout_key})\n",
    "    valid = (target_tokens > 0).astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    \n",
    "    logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "    token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(logits, axis=-1),\n",
    "            jnp.expand_dims(target_tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n",
    "    return token_log_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asetlur/.local/lib/python3.8/site-packages/jax/_src/interpreters/xla.py:176: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([-2.9248971e-04, -5.0683767e-01, -1.5708758e-04, -2.1932879e-05,\n",
       "       -6.4020278e-05, -1.5498859e-05], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_likelihood(unreplicate(train_state), input_tokens.reshape(-1, input_tokens.shape[-1]), target_tokens.reshape(-1, target_tokens.shape[-1]), keys_dropout[0])[0, train_data.num_prefix_tokens:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_tokens[0, train_data.num_prefix_tokens:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "sft_train_state = deepcopy(jax.device_put(train_state, device=jax.devices('cpu')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "repeat() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moriginal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: repeat() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "original[None, :, :].repeat(num_return_sequences, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 21.81it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9920, 35)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5,2|2,6|4,9|0,5|0,4|9,7/0,6=0,5,2,65,2|2,6|4,9|0,5|0,4|9,7/0,6=0,5,2,6',\n",
       " '5,2|2,6|4,9|0,5|0,4|9,7/0,6=0,5,2,65,2|2,6|4,9|0,5|0,4|9,7/0,6=0,4,9,7',\n",
       " '5,2|2,6|4,9|0,5|0,4|9,7/0,6=0,5,2,65,2|2,6|4,9|0,5|0,4|9,7/0,6=0,3,6,2',\n",
       " '5,2|2,6|4,9|0,5|0,4|9,7/0,6=0,5,2,65,2|2,6|4,9|0,5|0,4|9,7/0,6=0,0,5,2',\n",
       " '5,2|2,6|4,9|0,5|0,4|9,7/0,6=0,5,2,65,2|2,6|4,9|0,5|0,4|9,7/0,6=0,9,7,0',\n",
       " '7,5|2,7|3,1|9,6|9,2|6,3/9,1=9,6,3,17,5|2,7|3,1|9,6|9,2|6,3/9,1=9,2,7,5',\n",
       " '7,5|2,7|3,1|9,6|9,2|6,3/9,1=9,6,3,17,5|2,7|3,1|9,6|9,2|6,3/9,1=9,6,3,1',\n",
       " '7,5|2,7|3,1|9,6|9,2|6,3/9,1=9,6,3,17,5|2,7|3,1|9,6|9,2|6,3/9,1=9,3,1,9',\n",
       " '7,5|2,7|3,1|9,6|9,2|6,3/9,1=9,6,3,17,5|2,7|3,1|9,6|9,2|6,3/9,1=9,9,2,7',\n",
       " '7,5|2,7|3,1|9,6|9,2|6,3/9,1=9,6,3,17,5|2,7|3,1|9,6|9,2|6,3/9,1=9,1,7,5']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer.batch_decode(np.concatenate([original_dataset[10:20], generated_dataset[10:20]], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_level_scores(original_dataset, generated_dataset):\n",
    "    token_scores_arr = []\n",
    "    for i in range(original_dataset.shape[0]):\n",
    "        original_seq = original_dataset[i]\n",
    "        generated_seq = generated_dataset[i]\n",
    "        token_scores = (original_seq == generated_seq).float()\n",
    "        incorrect = torch.where(token_scores==0)\n",
    "        if len(incorrect[0]) > 0:\n",
    "            token_scores[incorrect[0][0].item()] = -1. \n",
    "            token_scores[incorrect[0][0].item()+1:] = 0. \n",
    "        token_scores[:train_data.num_prefix_tokens] = 0.\n",
    "        token_scores_arr.append(token_scores)\n",
    "    return torch.stack(token_scores_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 22.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9920, 35])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# token_lvl_scores.shape\n",
    "# A[10:20]\n",
    "# torch.where(A[1]==0)[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 21.94it/s]\n",
      "train loss: 0.03486354649066925 forcing train acc: 66.58678436279297 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [01:32<00:00,  1.67it/s]\n",
      "100%|██████████| 31/31 [00:01<00:00, 22.44it/s]\n",
      "train loss: 0.031758084893226624 forcing train acc: 66.65966796875 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 23.16it/s]   \n",
      "100%|██████████| 31/31 [00:01<00:00, 22.36it/s]\n",
      "train loss: 0.009832145646214485 forcing train acc: 66.4930191040039 policy train acc: 2.2177419662475586 policy test acc: 3.348214626312256: 100%|██████████| 155/155 [00:07<00:00, 21.77it/s] \n",
      "100%|██████████| 31/31 [00:01<00:00, 22.35it/s]\n",
      "train loss: 0.009871372953057289 forcing train acc: 65.42710876464844 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 22.95it/s]\n",
      "100%|██████████| 31/31 [00:01<00:00, 22.37it/s]\n",
      "train loss: 0.09410448372364044 forcing train acc: 66.65619659423828 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 22.88it/s]\n",
      "100%|██████████| 31/31 [00:01<00:00, 22.39it/s]\n",
      "train loss: 0.021869204938411713 forcing train acc: 66.6457748413086 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 22.91it/s] \n",
      "100%|██████████| 31/31 [00:01<00:00, 22.21it/s]\n",
      "train loss: 0.05177900940179825 forcing train acc: 66.26040649414062 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 22.86it/s] \n",
      "100%|██████████| 31/31 [00:01<00:00, 22.26it/s]\n",
      "train loss: 0.044199515134096146 forcing train acc: 53.5451545715332 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 23.02it/s] \n",
      "100%|██████████| 31/31 [00:01<00:00, 22.34it/s]\n",
      "train loss: 0.13039404153823853 forcing train acc: 57.826412200927734 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 22.91it/s]\n",
      "100%|██████████| 31/31 [00:01<00:00, 22.43it/s]\n",
      "train loss: 0.018449047580361366 forcing train acc: 59.347225189208984 policy train acc: 0.0 policy test acc: 0.0: 100%|██████████| 155/155 [00:06<00:00, 22.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def signed_log_sigmoid_loss_and_accuracy(logits, tokens, scores, valid=None):\n",
    "    if valid is None:\n",
    "        valid = jnp.ones(tokens.shape[:2])\n",
    "    valid = valid.astype(jnp.float32)\n",
    "    scores = scores.astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "    token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(logits, axis=-1),\n",
    "            jnp.expand_dims(tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    sign_token_log_prob = jnp.where(valid > 0.0, token_log_prob * scores, jnp.array(0.0))\n",
    "    # sign_seq_log_prob = jnp.sum(sign_token_log_prob, axis=-1) /valid_text_length\n",
    "    # # loss = -jax.nn.log_sigmoid(sign_seq_log_prob).mean()\n",
    "    # loss = -sign_seq_log_prob.mean()\n",
    "\n",
    "    # loss = -(sign_token_log_prob.sum(axis=-1)).mean()\n",
    "    loss = -jax.nn.log_sigmoid(sign_token_log_prob.sum(axis=-1) / valid_text_length).mean()\n",
    "\n",
    "    # old: loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n",
    "    # changed to match hf implementation\n",
    "    correct = jnp.where(\n",
    "        (valid > 0.0) & (scores > 0.0),\n",
    "        jnp.argmax(logits, axis=-1) == tokens,\n",
    "        jnp.array(False)\n",
    "    )\n",
    "    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n",
    "    return loss, accuracy\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0, 0))\n",
    "def train_step_onpolicy(state: TrainState, input_tokens: jnp.ndarray, target_tokens: jnp.ndarray, sign, dropout_key) -> Tuple[jnp.ndarray, TrainState]:\n",
    "    dropout_key = jax.random.fold_in(dropout_key, state.step)\n",
    "    sign = sign.reshape(-1)[:, None].repeat(input_tokens.shape[-1], 1)\n",
    "    def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
    "        logits = state.apply_fn(params, input_tokens, False, rngs={'dropout': dropout_key})\n",
    "        loss, acc = signed_log_sigmoid_loss_and_accuracy(\n",
    "            logits, target_tokens, scores, (scores != 0).astype(jnp.int32))\n",
    "        return loss, acc\n",
    "\n",
    "    # per-device loss and grads\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, acc), grads = grad_fn(state.params)\n",
    "    # average gradients across devices\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
    "    loss = jax.lax.pmean(loss, axis_name=\"batch\")\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return loss, acc, new_state\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    original_dataset = []\n",
    "    generated_dataset = []\n",
    "    # scores_dataset = []\n",
    "\n",
    "    for input_tokens, target_tokens in tqdm(iter(train_loader)): \n",
    "        input_tokens = jnp.array(input_tokens)\n",
    "        target_tokens = jnp.array(target_tokens)\n",
    "        original = jnp.concatenate([input_tokens[:, :train_data.num_prefix_tokens], target_tokens[:, -train_data.num_target_tokens:]], axis=1)\n",
    "        \n",
    "        input_tokens = input_tokens.reshape(jax.device_count(), -1, input_tokens.shape[-1])\n",
    "\n",
    "        generations = generate_negative_data(hf_params, train_state, input_tokens, key_gen)\n",
    "        \n",
    "        repeated_original = jnp.repeat(original[None, :, :], num_return_sequences, 0).transpose(1, 0, 2)\n",
    "        \n",
    "        original_dataset.append(repeated_original.reshape(-1, repeated_original.shape[-1]))\n",
    "        generated_dataset.append(generations[0].reshape(-1, generations[0].shape[-1]))\n",
    "        \n",
    "        # scores_dataset.append(generations[1].reshape(-1, generations[1].shape[-1]))\n",
    "\n",
    "    # scores_dataset = jnp.concatenate([x.reshape(-1) for x in scores_dataset], axis=0).squeeze()\n",
    "    generated_dataset = jnp.concatenate(generated_dataset, axis=0)\n",
    "    original_dataset = jnp.concatenate(original_dataset, axis=0)\n",
    "    token_lvl_scores = get_token_level_scores( torch.tensor(np.array(original_dataset)), torch.tensor(np.array(generated_dataset)))\n",
    "\n",
    "    on_policy_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(np.array(original_dataset)), torch.tensor(np.array(generated_dataset)), token_lvl_scores)\n",
    "\n",
    "    on_policy_loader = DataLoader(on_policy_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "    on_policy_iter = iter(on_policy_loader) \n",
    "\n",
    "    pbar = tqdm(on_policy_loader, total=len(on_policy_loader), desc='training on-policy')\n",
    "\n",
    "    train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter() \n",
    "    policy_train_acc, policy_test_acc = 0., 0.\n",
    "\n",
    "    step = 0.\n",
    "\n",
    "    for original, generation, scores in pbar:\n",
    "\n",
    "        input_tokens = original[:, :-1].clone()\n",
    "        # target_tokens = original.clone()\n",
    "        pred_tokens = generation.clone()\n",
    "        # target_tokens[:, :train_data.num_prefix_tokens] = -1\n",
    "        pred_tokens[:, :train_data.num_prefix_tokens] = -1\n",
    "        # target_tokens = target_tokens[:, 1:]\n",
    "        pred_tokens = pred_tokens[:, 1:]\n",
    "        # sign = 2 * (target_tokens[:, -1] == pred_tokens[:, -1]).float() - 1.\n",
    "\n",
    "        # input_tokens = torch.cat([input_tokens, input_tokens], axis=0)\n",
    "        # target_tokens = torch.cat([target_tokens, pred_tokens], axis=0)    \n",
    "        # sign = torch.cat([torch.ones(original.shape[0]), sign], axis=0)\n",
    "        \n",
    "        input_tokens = jnp.array(input_tokens)\n",
    "        pred_tokens = jnp.array(pred_tokens)\n",
    "        scores = jnp.array(scores[:, 1:])\n",
    "\n",
    "        input_tokens = input_tokens.reshape(jax.device_count(), -1, input_tokens.shape[-1])\n",
    "        pred_tokens = pred_tokens.reshape(jax.device_count(), -1, pred_tokens.shape[-1])\n",
    "        scores = scores.reshape(jax.device_count(), -1, scores.shape[-1])\n",
    "        \n",
    "        # print(input_tokens.shape, target_tokens.shape, scores.shape)\n",
    "\n",
    "        loss, acc, train_state = train_step_onpolicy(train_state, input_tokens, pred_tokens, scores, keys_dropout)\n",
    "        train_loss.update(loss.mean(), input_tokens.shape[1] * jax.device_count())  \n",
    "        train_acc.update(acc.mean(), input_tokens.shape[1] * jax.device_count())    \n",
    "        # phard.update(phard_token.mean(), input_tokens.shape[1] * jax.device_count())\n",
    "        if step % 10 == 0:\n",
    "            pbar.set_description(f'train loss: {train_loss.get()} forcing train acc: {train_acc.get(percentage=True)} policy train acc: {100. * policy_train_acc} policy test acc: {100. * policy_test_acc}')\n",
    "        if step % config.eval_interval == 0:\n",
    "            policy_train_acc = evaluate(hf_params, train_state, train_loader)\n",
    "            policy_test_acc = evaluate(hf_params, train_state, test_loader)\n",
    "            train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "        \n",
    "        step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 34), (64, 34))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hearth'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([31069])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 34)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.ones_like(inp[:train_data.num_prefix_tokens]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9,8|3,6|8,0|0,7|5,3|9,5/9,6= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,5|4,0|4,9|8,1|0,8|9,2/4,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,5|0,7|5,2|2,6|8,3|7,8/0,6= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '5,0|1,6|7,1|4,2|2,5|4,7/4,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,6|2,3|1,0|3,7|1,2|6,5/1,7= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '5,0|0,4|1,9|7,5|9,6|7,1/7,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '8,9|3,0|7,4|6,7|9,3|8,6/8,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '4,7|9,5|5,0|0,1|9,4|7,8/9,8= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,5|1,3|8,7|6,2|2,8|6,1/6,5= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,0|5,2|8,5|8,3|0,9|2,7/8,9= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '4,5|7,1|5,3|2,4|2,6|6,7/2,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,2|5,1|9,7|4,5|7,3|9,4/9,2= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '5,1|4,3|9,7|9,6|7,4|6,5/9,3= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,4|2,3|3,5|8,9|9,1|8,2/8,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,6|5,8|5,0|8,9|9,7|0,2/5,6= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,5|2,4|7,3|5,7|1,6|4,1/2,3= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,3|7,1|2,8|9,7|3,2|0,9/0,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,6|1,9|8,3|6,8|9,4|4,0/1,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '8,5|5,3|7,1|1,9|0,7|0,8/0,3= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,0|0,2|9,4|7,9|4,6|7,1/7,6= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,1|7,9|5,7|4,0|9,3|5,4/5,3= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '8,9|3,2|2,0|0,6|1,8|3,1/3,6= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,7|1,9|7,8|8,4|5,0|9,5/1,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '5,2|2,4|5,0|4,3|1,8|0,1/5,8= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,7|5,4|3,1|8,0|8,3|0,5/8,7= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,6|0,1|7,4|1,7|6,8|8,5/0,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '6,3|7,4|0,9|7,0|9,2|4,6/7,2= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,1|6,9|3,5|3,6|5,7|7,0/3,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '6,4|2,0|4,9|2,6|5,1|0,5/2,9= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,5|4,8|8,1|1,0|5,9|4,3/4,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,7|1,0|9,1|9,6|6,2|2,4/9,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '5,7|3,0|3,5|7,8|0,1|1,9/3,9= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '7,4|4,3|5,2|7,5|3,1|2,9/7,9= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '4,6|9,8|6,2|7,9|2,0|4,7/4,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,2|7,0|5,9|2,4|0,6|5,7/5,6= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,9|7,2|8,5|9,0|2,8|7,1/7,5= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,4|1,8|6,7|7,2|6,9|9,1/6,8= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '8,7|3,5|7,6|1,2|3,8|5,1/3,2= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,0|6,3|1,7|1,6|8,9|7,8/1,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '8,5|3,7|7,8|1,6|6,0|3,1/3,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '7,2|9,0|0,7|6,4|9,6|4,8/9,2= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,3|3,0|0,1|9,7|7,5|5,2/9,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,5|7,4|2,7|4,1|2,0|0,3/2,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,6|5,4|4,7|6,2|9,5|2,0/9,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,1|3,8|8,6|4,9|3,4|6,5/3,5= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '7,0|3,8|0,2|1,3|7,1|2,5/7,5=andraandraandraandraandraandraandra',\n",
       " '8,4|5,8|3,2|6,0|2,6|3,5/3,0= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,2|2,8|7,0|0,1|1,4|7,9/7,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,6|3,7|4,3|0,9|9,2|0,4/0,7= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,8|4,3|6,1|3,5|4,6|5,7/4,7= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,7|0,6|7,1|4,5|5,0|4,3/4,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '4,3|7,8|3,1|1,2|8,9|4,7/4,9= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '1,8|8,4|3,1|0,9|9,2|3,0/3,2= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '7,2|2,1|8,4|1,5|6,8|7,6/7,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '0,3|4,5|8,0|1,4|9,1|9,8/9,3= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,0|1,9|8,1|9,3|0,4|8,2/8,3= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,8|9,2|8,7|4,0|7,1|2,4/9,1= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '9,6|7,3|5,4|6,5|1,7|9,1/9,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '3,0|5,4|1,5|8,1|9,3|8,9/8,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '7,0|3,5|7,8|8,3|0,2|2,9/7,5= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '2,6|1,7|3,2|1,3|8,5|7,8/1,5= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '5,8|4,7|4,0|2,9|7,2|0,5/4,8= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '8,7|2,0|5,8|1,9|2,5|0,1/2,9= Hearth Hearth Hearth Hearth Hearth Hearth Hearth',\n",
       " '7,8|6,9|8,0|0,1|9,4|7,6/7,4= Hearth Hearth Hearth Hearth Hearth Hearth Hearth']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(frozen_dict_keys(['0', '1', '10', '11', '2', '3', '4', '5', '6', '7', '8', '9', 'ln_f', 'wpe', 'wte']),\n",
       " dict_keys(['h', 'ln_f', 'wpe', 'wte']))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.params['params'].keys(), hf['transformer'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.c_attn.bias ['transformer', 'h', '0', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.0.attn.c_attn.kernel ['transformer', 'h', '0', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.0.attn.c_proj.bias ['transformer', 'h', '0', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.0.attn.c_proj.kernel ['transformer', 'h', '0', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.0.ln_1.bias ['transformer', 'h', '0', 'ln_1', 'bias']\n",
      "transformer.h.0.ln_1.scale ['transformer', 'h', '0', 'ln_1', 'scale']\n",
      "transformer.h.0.ln_2.bias ['transformer', 'h', '0', 'ln_2', 'bias']\n",
      "transformer.h.0.ln_2.scale ['transformer', 'h', '0', 'ln_2', 'scale']\n",
      "transformer.h.0.mlp.c_fc.bias ['transformer', 'h', '0', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.0.mlp.c_fc.kernel ['transformer', 'h', '0', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.0.mlp.c_proj.bias ['transformer', 'h', '0', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.0.mlp.c_proj.kernel ['transformer', 'h', '0', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.1.attn.c_attn.bias ['transformer', 'h', '1', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.1.attn.c_attn.kernel ['transformer', 'h', '1', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.1.attn.c_proj.bias ['transformer', 'h', '1', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.1.attn.c_proj.kernel ['transformer', 'h', '1', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.1.ln_1.bias ['transformer', 'h', '1', 'ln_1', 'bias']\n",
      "transformer.h.1.ln_1.scale ['transformer', 'h', '1', 'ln_1', 'scale']\n",
      "transformer.h.1.ln_2.bias ['transformer', 'h', '1', 'ln_2', 'bias']\n",
      "transformer.h.1.ln_2.scale ['transformer', 'h', '1', 'ln_2', 'scale']\n",
      "transformer.h.1.mlp.c_fc.bias ['transformer', 'h', '1', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.1.mlp.c_fc.kernel ['transformer', 'h', '1', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.1.mlp.c_proj.bias ['transformer', 'h', '1', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.1.mlp.c_proj.kernel ['transformer', 'h', '1', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.10.attn.c_attn.bias ['transformer', 'h', '10', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.10.attn.c_attn.kernel ['transformer', 'h', '10', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.10.attn.c_proj.bias ['transformer', 'h', '10', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.10.attn.c_proj.kernel ['transformer', 'h', '10', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.10.ln_1.bias ['transformer', 'h', '10', 'ln_1', 'bias']\n",
      "transformer.h.10.ln_1.scale ['transformer', 'h', '10', 'ln_1', 'scale']\n",
      "transformer.h.10.ln_2.bias ['transformer', 'h', '10', 'ln_2', 'bias']\n",
      "transformer.h.10.ln_2.scale ['transformer', 'h', '10', 'ln_2', 'scale']\n",
      "transformer.h.10.mlp.c_fc.bias ['transformer', 'h', '10', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.10.mlp.c_fc.kernel ['transformer', 'h', '10', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.10.mlp.c_proj.bias ['transformer', 'h', '10', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.10.mlp.c_proj.kernel ['transformer', 'h', '10', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.11.attn.c_attn.bias ['transformer', 'h', '11', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.11.attn.c_attn.kernel ['transformer', 'h', '11', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.11.attn.c_proj.bias ['transformer', 'h', '11', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.11.attn.c_proj.kernel ['transformer', 'h', '11', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.11.ln_1.bias ['transformer', 'h', '11', 'ln_1', 'bias']\n",
      "transformer.h.11.ln_1.scale ['transformer', 'h', '11', 'ln_1', 'scale']\n",
      "transformer.h.11.ln_2.bias ['transformer', 'h', '11', 'ln_2', 'bias']\n",
      "transformer.h.11.ln_2.scale ['transformer', 'h', '11', 'ln_2', 'scale']\n",
      "transformer.h.11.mlp.c_fc.bias ['transformer', 'h', '11', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.11.mlp.c_fc.kernel ['transformer', 'h', '11', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.11.mlp.c_proj.bias ['transformer', 'h', '11', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.11.mlp.c_proj.kernel ['transformer', 'h', '11', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.2.attn.c_attn.bias ['transformer', 'h', '2', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.2.attn.c_attn.kernel ['transformer', 'h', '2', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.2.attn.c_proj.bias ['transformer', 'h', '2', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.2.attn.c_proj.kernel ['transformer', 'h', '2', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.2.ln_1.bias ['transformer', 'h', '2', 'ln_1', 'bias']\n",
      "transformer.h.2.ln_1.scale ['transformer', 'h', '2', 'ln_1', 'scale']\n",
      "transformer.h.2.ln_2.bias ['transformer', 'h', '2', 'ln_2', 'bias']\n",
      "transformer.h.2.ln_2.scale ['transformer', 'h', '2', 'ln_2', 'scale']\n",
      "transformer.h.2.mlp.c_fc.bias ['transformer', 'h', '2', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.2.mlp.c_fc.kernel ['transformer', 'h', '2', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.2.mlp.c_proj.bias ['transformer', 'h', '2', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.2.mlp.c_proj.kernel ['transformer', 'h', '2', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.3.attn.c_attn.bias ['transformer', 'h', '3', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.3.attn.c_attn.kernel ['transformer', 'h', '3', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.3.attn.c_proj.bias ['transformer', 'h', '3', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.3.attn.c_proj.kernel ['transformer', 'h', '3', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.3.ln_1.bias ['transformer', 'h', '3', 'ln_1', 'bias']\n",
      "transformer.h.3.ln_1.scale ['transformer', 'h', '3', 'ln_1', 'scale']\n",
      "transformer.h.3.ln_2.bias ['transformer', 'h', '3', 'ln_2', 'bias']\n",
      "transformer.h.3.ln_2.scale ['transformer', 'h', '3', 'ln_2', 'scale']\n",
      "transformer.h.3.mlp.c_fc.bias ['transformer', 'h', '3', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.3.mlp.c_fc.kernel ['transformer', 'h', '3', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.3.mlp.c_proj.bias ['transformer', 'h', '3', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.3.mlp.c_proj.kernel ['transformer', 'h', '3', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.4.attn.c_attn.bias ['transformer', 'h', '4', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.4.attn.c_attn.kernel ['transformer', 'h', '4', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.4.attn.c_proj.bias ['transformer', 'h', '4', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.4.attn.c_proj.kernel ['transformer', 'h', '4', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.4.ln_1.bias ['transformer', 'h', '4', 'ln_1', 'bias']\n",
      "transformer.h.4.ln_1.scale ['transformer', 'h', '4', 'ln_1', 'scale']\n",
      "transformer.h.4.ln_2.bias ['transformer', 'h', '4', 'ln_2', 'bias']\n",
      "transformer.h.4.ln_2.scale ['transformer', 'h', '4', 'ln_2', 'scale']\n",
      "transformer.h.4.mlp.c_fc.bias ['transformer', 'h', '4', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.4.mlp.c_fc.kernel ['transformer', 'h', '4', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.4.mlp.c_proj.bias ['transformer', 'h', '4', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.4.mlp.c_proj.kernel ['transformer', 'h', '4', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.5.attn.c_attn.bias ['transformer', 'h', '5', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.5.attn.c_attn.kernel ['transformer', 'h', '5', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.5.attn.c_proj.bias ['transformer', 'h', '5', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.5.attn.c_proj.kernel ['transformer', 'h', '5', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.5.ln_1.bias ['transformer', 'h', '5', 'ln_1', 'bias']\n",
      "transformer.h.5.ln_1.scale ['transformer', 'h', '5', 'ln_1', 'scale']\n",
      "transformer.h.5.ln_2.bias ['transformer', 'h', '5', 'ln_2', 'bias']\n",
      "transformer.h.5.ln_2.scale ['transformer', 'h', '5', 'ln_2', 'scale']\n",
      "transformer.h.5.mlp.c_fc.bias ['transformer', 'h', '5', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.5.mlp.c_fc.kernel ['transformer', 'h', '5', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.5.mlp.c_proj.bias ['transformer', 'h', '5', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.5.mlp.c_proj.kernel ['transformer', 'h', '5', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.6.attn.c_attn.bias ['transformer', 'h', '6', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.6.attn.c_attn.kernel ['transformer', 'h', '6', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.6.attn.c_proj.bias ['transformer', 'h', '6', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.6.attn.c_proj.kernel ['transformer', 'h', '6', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.6.ln_1.bias ['transformer', 'h', '6', 'ln_1', 'bias']\n",
      "transformer.h.6.ln_1.scale ['transformer', 'h', '6', 'ln_1', 'scale']\n",
      "transformer.h.6.ln_2.bias ['transformer', 'h', '6', 'ln_2', 'bias']\n",
      "transformer.h.6.ln_2.scale ['transformer', 'h', '6', 'ln_2', 'scale']\n",
      "transformer.h.6.mlp.c_fc.bias ['transformer', 'h', '6', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.6.mlp.c_fc.kernel ['transformer', 'h', '6', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.6.mlp.c_proj.bias ['transformer', 'h', '6', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.6.mlp.c_proj.kernel ['transformer', 'h', '6', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.7.attn.c_attn.bias ['transformer', 'h', '7', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.7.attn.c_attn.kernel ['transformer', 'h', '7', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.7.attn.c_proj.bias ['transformer', 'h', '7', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.7.attn.c_proj.kernel ['transformer', 'h', '7', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.7.ln_1.bias ['transformer', 'h', '7', 'ln_1', 'bias']\n",
      "transformer.h.7.ln_1.scale ['transformer', 'h', '7', 'ln_1', 'scale']\n",
      "transformer.h.7.ln_2.bias ['transformer', 'h', '7', 'ln_2', 'bias']\n",
      "transformer.h.7.ln_2.scale ['transformer', 'h', '7', 'ln_2', 'scale']\n",
      "transformer.h.7.mlp.c_fc.bias ['transformer', 'h', '7', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.7.mlp.c_fc.kernel ['transformer', 'h', '7', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.7.mlp.c_proj.bias ['transformer', 'h', '7', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.7.mlp.c_proj.kernel ['transformer', 'h', '7', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.8.attn.c_attn.bias ['transformer', 'h', '8', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.8.attn.c_attn.kernel ['transformer', 'h', '8', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.8.attn.c_proj.bias ['transformer', 'h', '8', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.8.attn.c_proj.kernel ['transformer', 'h', '8', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.8.ln_1.bias ['transformer', 'h', '8', 'ln_1', 'bias']\n",
      "transformer.h.8.ln_1.scale ['transformer', 'h', '8', 'ln_1', 'scale']\n",
      "transformer.h.8.ln_2.bias ['transformer', 'h', '8', 'ln_2', 'bias']\n",
      "transformer.h.8.ln_2.scale ['transformer', 'h', '8', 'ln_2', 'scale']\n",
      "transformer.h.8.mlp.c_fc.bias ['transformer', 'h', '8', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.8.mlp.c_fc.kernel ['transformer', 'h', '8', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.8.mlp.c_proj.bias ['transformer', 'h', '8', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.8.mlp.c_proj.kernel ['transformer', 'h', '8', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.9.attn.c_attn.bias ['transformer', 'h', '9', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.9.attn.c_attn.kernel ['transformer', 'h', '9', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.9.attn.c_proj.bias ['transformer', 'h', '9', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.9.attn.c_proj.kernel ['transformer', 'h', '9', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.9.ln_1.bias ['transformer', 'h', '9', 'ln_1', 'bias']\n",
      "transformer.h.9.ln_1.scale ['transformer', 'h', '9', 'ln_1', 'scale']\n",
      "transformer.h.9.ln_2.bias ['transformer', 'h', '9', 'ln_2', 'bias']\n",
      "transformer.h.9.ln_2.scale ['transformer', 'h', '9', 'ln_2', 'scale']\n",
      "transformer.h.9.mlp.c_fc.bias ['transformer', 'h', '9', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.9.mlp.c_fc.kernel ['transformer', 'h', '9', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.9.mlp.c_proj.bias ['transformer', 'h', '9', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.9.mlp.c_proj.kernel ['transformer', 'h', '9', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.ln_f.bias ['transformer', 'ln_f', 'bias']\n",
      "transformer.ln_f.scale ['transformer', 'ln_f', 'scale']\n",
      "transformer.wpe.embedding ['transformer', 'wpe', 'embedding']\n",
      "transformer.wte.embedding ['transformer', 'wte', 'embedding']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m hf_conv \u001b[38;5;241m=\u001b[39m convert_jax_params_to_hf(hf, s\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m s\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(), hf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(), \u001b[43mhf_conv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/flax/core/frozen_dict.py:66\u001b[0m, in \u001b[0;36mFrozenDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m---> 66\u001b[0m   v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     67\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FrozenDict(v)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'params'"
     ]
    }
   ],
   "source": [
    "hf_conv = convert_jax_params_to_hf(hf, s.params['params'])\n",
    "s.params['params'].keys(), hf['transformer'].keys(), hf_conv['params']['transformer'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.c_attn.bias ['transformer', 'h', '0', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.0.attn.c_attn.kernel ['transformer', 'h', '0', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.0.attn.c_proj.bias ['transformer', 'h', '0', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.0.attn.c_proj.kernel ['transformer', 'h', '0', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.0.ln_1.bias ['transformer', 'h', '0', 'ln_1', 'bias']\n",
      "transformer.h.0.ln_1.scale ['transformer', 'h', '0', 'ln_1', 'scale']\n",
      "transformer.h.0.ln_2.bias ['transformer', 'h', '0', 'ln_2', 'bias']\n",
      "transformer.h.0.ln_2.scale ['transformer', 'h', '0', 'ln_2', 'scale']\n",
      "transformer.h.0.mlp.c_fc.bias ['transformer', 'h', '0', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.0.mlp.c_fc.kernel ['transformer', 'h', '0', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.0.mlp.c_proj.bias ['transformer', 'h', '0', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.0.mlp.c_proj.kernel ['transformer', 'h', '0', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.1.attn.c_attn.bias ['transformer', 'h', '1', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.1.attn.c_attn.kernel ['transformer', 'h', '1', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.1.attn.c_proj.bias ['transformer', 'h', '1', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.1.attn.c_proj.kernel ['transformer', 'h', '1', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.1.ln_1.bias ['transformer', 'h', '1', 'ln_1', 'bias']\n",
      "transformer.h.1.ln_1.scale ['transformer', 'h', '1', 'ln_1', 'scale']\n",
      "transformer.h.1.ln_2.bias ['transformer', 'h', '1', 'ln_2', 'bias']\n",
      "transformer.h.1.ln_2.scale ['transformer', 'h', '1', 'ln_2', 'scale']\n",
      "transformer.h.1.mlp.c_fc.bias ['transformer', 'h', '1', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.1.mlp.c_fc.kernel ['transformer', 'h', '1', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.1.mlp.c_proj.bias ['transformer', 'h', '1', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.1.mlp.c_proj.kernel ['transformer', 'h', '1', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.10.attn.c_attn.bias ['transformer', 'h', '10', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.10.attn.c_attn.kernel ['transformer', 'h', '10', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.10.attn.c_proj.bias ['transformer', 'h', '10', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.10.attn.c_proj.kernel ['transformer', 'h', '10', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.10.ln_1.bias ['transformer', 'h', '10', 'ln_1', 'bias']\n",
      "transformer.h.10.ln_1.scale ['transformer', 'h', '10', 'ln_1', 'scale']\n",
      "transformer.h.10.ln_2.bias ['transformer', 'h', '10', 'ln_2', 'bias']\n",
      "transformer.h.10.ln_2.scale ['transformer', 'h', '10', 'ln_2', 'scale']\n",
      "transformer.h.10.mlp.c_fc.bias ['transformer', 'h', '10', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.10.mlp.c_fc.kernel ['transformer', 'h', '10', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.10.mlp.c_proj.bias ['transformer', 'h', '10', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.10.mlp.c_proj.kernel ['transformer', 'h', '10', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.11.attn.c_attn.bias ['transformer', 'h', '11', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.11.attn.c_attn.kernel ['transformer', 'h', '11', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.11.attn.c_proj.bias ['transformer', 'h', '11', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.11.attn.c_proj.kernel ['transformer', 'h', '11', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.11.ln_1.bias ['transformer', 'h', '11', 'ln_1', 'bias']\n",
      "transformer.h.11.ln_1.scale ['transformer', 'h', '11', 'ln_1', 'scale']\n",
      "transformer.h.11.ln_2.bias ['transformer', 'h', '11', 'ln_2', 'bias']\n",
      "transformer.h.11.ln_2.scale ['transformer', 'h', '11', 'ln_2', 'scale']\n",
      "transformer.h.11.mlp.c_fc.bias ['transformer', 'h', '11', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.11.mlp.c_fc.kernel ['transformer', 'h', '11', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.11.mlp.c_proj.bias ['transformer', 'h', '11', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.11.mlp.c_proj.kernel ['transformer', 'h', '11', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.2.attn.c_attn.bias ['transformer', 'h', '2', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.2.attn.c_attn.kernel ['transformer', 'h', '2', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.2.attn.c_proj.bias ['transformer', 'h', '2', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.2.attn.c_proj.kernel ['transformer', 'h', '2', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.2.ln_1.bias ['transformer', 'h', '2', 'ln_1', 'bias']\n",
      "transformer.h.2.ln_1.scale ['transformer', 'h', '2', 'ln_1', 'scale']\n",
      "transformer.h.2.ln_2.bias ['transformer', 'h', '2', 'ln_2', 'bias']\n",
      "transformer.h.2.ln_2.scale ['transformer', 'h', '2', 'ln_2', 'scale']\n",
      "transformer.h.2.mlp.c_fc.bias ['transformer', 'h', '2', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.2.mlp.c_fc.kernel ['transformer', 'h', '2', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.2.mlp.c_proj.bias ['transformer', 'h', '2', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.2.mlp.c_proj.kernel ['transformer', 'h', '2', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.3.attn.c_attn.bias ['transformer', 'h', '3', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.3.attn.c_attn.kernel ['transformer', 'h', '3', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.3.attn.c_proj.bias ['transformer', 'h', '3', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.3.attn.c_proj.kernel ['transformer', 'h', '3', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.3.ln_1.bias ['transformer', 'h', '3', 'ln_1', 'bias']\n",
      "transformer.h.3.ln_1.scale ['transformer', 'h', '3', 'ln_1', 'scale']\n",
      "transformer.h.3.ln_2.bias ['transformer', 'h', '3', 'ln_2', 'bias']\n",
      "transformer.h.3.ln_2.scale ['transformer', 'h', '3', 'ln_2', 'scale']\n",
      "transformer.h.3.mlp.c_fc.bias ['transformer', 'h', '3', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.3.mlp.c_fc.kernel ['transformer', 'h', '3', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.3.mlp.c_proj.bias ['transformer', 'h', '3', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.3.mlp.c_proj.kernel ['transformer', 'h', '3', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.4.attn.c_attn.bias ['transformer', 'h', '4', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.4.attn.c_attn.kernel ['transformer', 'h', '4', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.4.attn.c_proj.bias ['transformer', 'h', '4', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.4.attn.c_proj.kernel ['transformer', 'h', '4', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.4.ln_1.bias ['transformer', 'h', '4', 'ln_1', 'bias']\n",
      "transformer.h.4.ln_1.scale ['transformer', 'h', '4', 'ln_1', 'scale']\n",
      "transformer.h.4.ln_2.bias ['transformer', 'h', '4', 'ln_2', 'bias']\n",
      "transformer.h.4.ln_2.scale ['transformer', 'h', '4', 'ln_2', 'scale']\n",
      "transformer.h.4.mlp.c_fc.bias ['transformer', 'h', '4', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.4.mlp.c_fc.kernel ['transformer', 'h', '4', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.4.mlp.c_proj.bias ['transformer', 'h', '4', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.4.mlp.c_proj.kernel ['transformer', 'h', '4', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.5.attn.c_attn.bias ['transformer', 'h', '5', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.5.attn.c_attn.kernel ['transformer', 'h', '5', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.5.attn.c_proj.bias ['transformer', 'h', '5', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.5.attn.c_proj.kernel ['transformer', 'h', '5', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.5.ln_1.bias ['transformer', 'h', '5', 'ln_1', 'bias']\n",
      "transformer.h.5.ln_1.scale ['transformer', 'h', '5', 'ln_1', 'scale']\n",
      "transformer.h.5.ln_2.bias ['transformer', 'h', '5', 'ln_2', 'bias']\n",
      "transformer.h.5.ln_2.scale ['transformer', 'h', '5', 'ln_2', 'scale']\n",
      "transformer.h.5.mlp.c_fc.bias ['transformer', 'h', '5', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.5.mlp.c_fc.kernel ['transformer', 'h', '5', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.5.mlp.c_proj.bias ['transformer', 'h', '5', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.5.mlp.c_proj.kernel ['transformer', 'h', '5', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.6.attn.c_attn.bias ['transformer', 'h', '6', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.6.attn.c_attn.kernel ['transformer', 'h', '6', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.6.attn.c_proj.bias ['transformer', 'h', '6', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.6.attn.c_proj.kernel ['transformer', 'h', '6', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.6.ln_1.bias ['transformer', 'h', '6', 'ln_1', 'bias']\n",
      "transformer.h.6.ln_1.scale ['transformer', 'h', '6', 'ln_1', 'scale']\n",
      "transformer.h.6.ln_2.bias ['transformer', 'h', '6', 'ln_2', 'bias']\n",
      "transformer.h.6.ln_2.scale ['transformer', 'h', '6', 'ln_2', 'scale']\n",
      "transformer.h.6.mlp.c_fc.bias ['transformer', 'h', '6', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.6.mlp.c_fc.kernel ['transformer', 'h', '6', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.6.mlp.c_proj.bias ['transformer', 'h', '6', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.6.mlp.c_proj.kernel ['transformer', 'h', '6', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.7.attn.c_attn.bias ['transformer', 'h', '7', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.7.attn.c_attn.kernel ['transformer', 'h', '7', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.7.attn.c_proj.bias ['transformer', 'h', '7', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.7.attn.c_proj.kernel ['transformer', 'h', '7', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.7.ln_1.bias ['transformer', 'h', '7', 'ln_1', 'bias']\n",
      "transformer.h.7.ln_1.scale ['transformer', 'h', '7', 'ln_1', 'scale']\n",
      "transformer.h.7.ln_2.bias ['transformer', 'h', '7', 'ln_2', 'bias']\n",
      "transformer.h.7.ln_2.scale ['transformer', 'h', '7', 'ln_2', 'scale']\n",
      "transformer.h.7.mlp.c_fc.bias ['transformer', 'h', '7', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.7.mlp.c_fc.kernel ['transformer', 'h', '7', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.7.mlp.c_proj.bias ['transformer', 'h', '7', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.7.mlp.c_proj.kernel ['transformer', 'h', '7', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.8.attn.c_attn.bias ['transformer', 'h', '8', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.8.attn.c_attn.kernel ['transformer', 'h', '8', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.8.attn.c_proj.bias ['transformer', 'h', '8', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.8.attn.c_proj.kernel ['transformer', 'h', '8', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.8.ln_1.bias ['transformer', 'h', '8', 'ln_1', 'bias']\n",
      "transformer.h.8.ln_1.scale ['transformer', 'h', '8', 'ln_1', 'scale']\n",
      "transformer.h.8.ln_2.bias ['transformer', 'h', '8', 'ln_2', 'bias']\n",
      "transformer.h.8.ln_2.scale ['transformer', 'h', '8', 'ln_2', 'scale']\n",
      "transformer.h.8.mlp.c_fc.bias ['transformer', 'h', '8', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.8.mlp.c_fc.kernel ['transformer', 'h', '8', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.8.mlp.c_proj.bias ['transformer', 'h', '8', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.8.mlp.c_proj.kernel ['transformer', 'h', '8', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.h.9.attn.c_attn.bias ['transformer', 'h', '9', 'attn', 'c_attn', 'bias']\n",
      "transformer.h.9.attn.c_attn.kernel ['transformer', 'h', '9', 'attn', 'c_attn', 'kernel']\n",
      "transformer.h.9.attn.c_proj.bias ['transformer', 'h', '9', 'attn', 'c_proj', 'bias']\n",
      "transformer.h.9.attn.c_proj.kernel ['transformer', 'h', '9', 'attn', 'c_proj', 'kernel']\n",
      "transformer.h.9.ln_1.bias ['transformer', 'h', '9', 'ln_1', 'bias']\n",
      "transformer.h.9.ln_1.scale ['transformer', 'h', '9', 'ln_1', 'scale']\n",
      "transformer.h.9.ln_2.bias ['transformer', 'h', '9', 'ln_2', 'bias']\n",
      "transformer.h.9.ln_2.scale ['transformer', 'h', '9', 'ln_2', 'scale']\n",
      "transformer.h.9.mlp.c_fc.bias ['transformer', 'h', '9', 'mlp', 'c_fc', 'bias']\n",
      "transformer.h.9.mlp.c_fc.kernel ['transformer', 'h', '9', 'mlp', 'c_fc', 'kernel']\n",
      "transformer.h.9.mlp.c_proj.bias ['transformer', 'h', '9', 'mlp', 'c_proj', 'bias']\n",
      "transformer.h.9.mlp.c_proj.kernel ['transformer', 'h', '9', 'mlp', 'c_proj', 'kernel']\n",
      "transformer.ln_f.bias ['transformer', 'ln_f', 'bias']\n",
      "transformer.ln_f.scale ['transformer', 'ln_f', 'scale']\n",
      "transformer.wpe.embedding ['transformer', 'wpe', 'embedding']\n",
      "transformer.wte.embedding ['transformer', 'wte', 'embedding']\n",
      "[[91 19 11 20 14 24 11]\n",
      " [16 11 17 14 19 11 18]\n",
      " [91 15 11 21 28 91 15]\n",
      " [91 16 11 15 91 16 11]\n",
      " [91 17 11 18 91 16 11]\n",
      " [91 16 11 17 91 16 11]\n",
      " [91 18 11 15 91 17 11]\n",
      " [91 22 11 22 91 21 11]\n",
      " [91 16 11 17 14 21 11]\n",
      " [91 17 11 22 14 23 11]\n",
      " [91 17 11 17 28 91 18]\n",
      " [91 19 11 17 91 19 11]\n",
      " [91 19 11 18 28 91 19]\n",
      " [16 11 17 14 23 11 19]\n",
      " [91 20 11 21 91 20 11]\n",
      " [16 11 17 28 16 11 18]\n",
      " [15 11 17 28 15 11 18]\n",
      " [91 15 11 15 11 15 91]\n",
      " [15 11 19 28 15 11 20]\n",
      " [91 16 11 15 91 15 11]\n",
      " [91 19 11 15 91 19 11]\n",
      " [91 17 11 15 91 17 11]\n",
      " [16 11 17 28 16 11 18]\n",
      " [91 16 11 23 91 15 11]\n",
      " [91 20 11 19 91 15 11]\n",
      " [15 11 15 11 15 11 15]\n",
      " [91 19 11 21 14 22 11]\n",
      " [91 15 11 15 11 15 91]\n",
      " [91 16 11 16 11 16 91]\n",
      " [91 19 11 15 91 18 11]\n",
      " [91 17 11 19 14 24 11]\n",
      " [91 16 11 24 14 18 11]\n",
      " [91 22 11 24 28 91 22]\n",
      " [91 19 11 15 28 91 19]\n",
      " [91 21 11 15 91 21 11]\n",
      " [91 22 11 16 14 22 11]\n",
      " [91 16 11 23 28 91 16]\n",
      " [16 11 18 28 16 11 19]\n",
      " [91 22 11 15 91 22 11]\n",
      " [91 17 11 15 14 18 11]\n",
      " [91 19 11 17 91 19 11]\n",
      " [91 17 11 16 91 17 11]\n",
      " [16 11 17 28 16 11 18]\n",
      " [91 15 11 15 11 15 91]\n",
      " [91 18 11 19 91 18 11]\n",
      " [91 17 11 20 14 22 11]\n",
      " [91 17 11 15 91 17 11]\n",
      " [91 17 11 23 91 17 11]\n",
      " [15 11 15 11 15 11 15]\n",
      " [91 19 11 20 91 19 11]\n",
      " [16 11 17 28 16 11 18]\n",
      " [91 19 11 23 91 19 11]\n",
      " [91 18 11 15 14 18 11]\n",
      " [91 21 11 19 91 21 11]\n",
      " [91 19 11 20 91 21 11]\n",
      " [91 16 11 17 14 23 11]\n",
      " [16 11 17 28 16 11 18]\n",
      " [16 11 17 14 24 11 18]\n",
      " [91 19 11 17 11 15 91]\n",
      " [91 17 11 24 14 22 11]\n",
      " [91 16 11 17 91 16 11]\n",
      " [91 19 11 23 91 19 11]\n",
      " [91 16 11 24 91 17 11]\n",
      " [91 21 11 19 91 21 11]] [[24 11 20 11 18 11 21]\n",
      " [19 11 15 11 23 11 16]\n",
      " [15 11 20 11 17 11 21]\n",
      " [19 11 17 11 20 11 15]\n",
      " [16 11 17 11 18 11 22]\n",
      " [22 11 20 11 15 11 19]\n",
      " [23 11 21 11 22 11 19]\n",
      " [24 11 19 11 22 11 23]\n",
      " [21 11 16 11 18 11 20]\n",
      " [23 11 18 11 15 11 24]\n",
      " [17 11 21 11 22 11 16]\n",
      " [24 11 22 11 18 11 17]\n",
      " [24 11 22 11 19 11 18]\n",
      " [23 11 24 11 16 11 19]\n",
      " [20 11 15 11 17 11 21]\n",
      " [17 11 20 11 22 11 18]\n",
      " [15 11 24 11 22 11 16]\n",
      " [16 11 24 11 19 11 15]\n",
      " [15 11 23 11 20 11 18]\n",
      " [22 11 24 11 19 11 21]\n",
      " [20 11 22 11 24 11 18]\n",
      " [18 11 17 11 15 11 21]\n",
      " [16 11 22 11 23 11 19]\n",
      " [20 11 15 11 16 11 23]\n",
      " [23 11 18 11 16 11 22]\n",
      " [15 11 16 11 22 11 19]\n",
      " [22 11 15 11 24 11 17]\n",
      " [18 11 20 11 22 11 15]\n",
      " [17 11 21 11 19 11 24]\n",
      " [19 11 23 11 16 11 15]\n",
      " [24 11 21 11 17 11 19]\n",
      " [18 11 15 11 16 11 24]\n",
      " [22 11 20 11 17 11 24]\n",
      " [19 11 21 11 17 11 15]\n",
      " [20 11 22 11 15 11 21]\n",
      " [22 11 17 11 23 11 20]\n",
      " [21 11 24 11 16 11 23]\n",
      " [18 11 20 11 16 11 17]\n",
      " [16 11 21 11 18 11 15]\n",
      " [18 11 16 11 21 11 15]\n",
      " [24 11 15 11 22 11 17]\n",
      " [24 11 18 11 15 11 16]\n",
      " [17 11 22 11 19 11 16]\n",
      " [24 11 21 11 17 11 15]\n",
      " [18 11 23 11 21 11 20]\n",
      " [22 11 15 11 17 11 20]\n",
      " [18 11 17 11 21 11 15]\n",
      " [22 11 15 11 16 11 19]\n",
      " [15 11 19 11 18 11 22]\n",
      " [19 11 18 11 20 11 22]\n",
      " [19 11 18 11 22 11 16]\n",
      " [19 11 22 11 23 11 24]\n",
      " [18 11 15 11 24 11 17]\n",
      " [22 11 21 11 23 11 19]\n",
      " [24 11 23 11 15 11 18]\n",
      " [23 11 16 11 24 11 18]\n",
      " [24 11 23 11 22 11 16]\n",
      " [24 11 21 11 20 11 19]\n",
      " [23 11 16 11 20 11 19]\n",
      " [22 11 23 11 18 11 20]\n",
      " [16 11 22 11 23 11 20]\n",
      " [19 11 15 11 20 11 23]\n",
      " [17 11 15 11 16 11 24]\n",
      " [22 11 21 11 24 11 19]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(64, 7), (4, 7, 34)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/util.py:284\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/util.py:277\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/lax/lax.py:153\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/lax/lax.py:169\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64, 7), (4, 7, 34)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m hf_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     12\u001b[0m     inp[:, :train_data\u001b[38;5;241m.\u001b[39mnum_prefix_tokens], params\u001b[38;5;241m=\u001b[39mhf_conv, max_new_tokens\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mnum_target_tokens, min_length\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mnum_target_tokens\u001b[38;5;241m+\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mnum_prefix_tokens, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mones_like(inp[:, :train_data\u001b[38;5;241m.\u001b[39mnum_prefix_tokens]))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mnum_target_tokens:], t[:, \u001b[38;5;241m-\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mnum_target_tokens:])\n\u001b[0;32m---> 14\u001b[0m acc \u001b[38;5;241m=\u001b[39m ((\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_target_tokens\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_target_tokens\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m train_data\u001b[38;5;241m.\u001b[39mnum_target_tokens)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     15\u001b[0m acc \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mpmean(acc, axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/array_methods.py:258\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    256\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m    260\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported operand type(s) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/ufuncs.py:82\u001b[0m, in \u001b[0;36m_one_to_one_binop.<locals>.<lambda>\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     80\u001b[0m   fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2, \u001b[38;5;241m/\u001b[39m: lax_fn(\u001b[38;5;241m*\u001b[39mpromote_args_numeric(numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, x1, x2))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m   fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2, \u001b[38;5;241m/\u001b[39m: lax_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     83\u001b[0m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.numpy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m fn \u001b[38;5;241m=\u001b[39m jit(fn, inline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/util.py:357\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    355\u001b[0m check_arraylike(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    356\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/util.py:247\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_numpy_rank_promotion \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    246\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 247\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/lax/lax.py:169\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64, 7), (4, 7, 34)]"
     ]
    }
   ],
   "source": [
    "# evaluate(hf_params, train_state, train_loader)\n",
    "# eval_step(hf_params, train_state, input_tokens, target_tokens)\n",
    "inp = input_tokens.reshape(-1, input_tokens.shape[-1])\n",
    "t = target_tokens.reshape(-1, target_tokens.shape[-1])\n",
    "\n",
    "# hf = unfreeze(unreplicate(hf_params))\n",
    "# s = unreplicate(train_state)\n",
    "hf_conv = convert_jax_params_to_hf(hf_params, train_state.params['params'])\n",
    "train_state.params['params'].keys(), hf_params['transformer'].keys(), hf_conv['transformer'].keys()\n",
    "\n",
    "output = hf_model.generate(\n",
    "    inp[:, :train_data.num_prefix_tokens], params=hf_conv, max_new_tokens=train_data.num_target_tokens, min_length=train_data.num_target_tokens+train_data.num_prefix_tokens, do_sample=False, attention_mask=jnp.ones_like(inp[:, :train_data.num_prefix_tokens]))\n",
    "print(output[0][:, -train_data.num_target_tokens:], t[:, -train_data.num_target_tokens:])\n",
    "acc = ((output[0][:, -train_data.num_target_tokens:] == target_tokens[:, -train_data.num_target_tokens:]).sum(1) == train_data.num_target_tokens).mean()\n",
    "acc = jax.lax.pmean(acc, axis_name=\"batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['k'])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_conv.keys(   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2304, 768), (2304, 768))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_conv['params']['transformer']['h']['0']['attn']['c_attn']['kernel'].shape, hf_params['transformer']['h']['0']['attn']['c_attn']['kernel'].shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = input_tokens.reshape(-1, input_tokens.shape[-1])\n",
    "target_tokens = target_tokens.reshape(-1, target_tokens.shape[-1])\n",
    "hf_params = unfreeze(hf_params)\n",
    "for k in ('ln_f', 'wpe', 'wte'):\n",
    "    hf_params['transformer'][k] = train_state.params['params'][k]\n",
    "hf_model.generate(\n",
    "    input_tokens, params=hf_params, max_new_tokens=max_new_tokens, prng_key=key, num_beams=5, num_return_sequences=5, temperature=1.0, attention_mask=jnp.ones_like(input_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 31/31 [01:13<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9920,), (9920, 29), (9920, 29))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dataset.shape, generated_dataset.shape, original_dataset.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_log_sigmoid_loss_and_accuracy(logits, tokens, sign, valid=None):\n",
    "    if valid is None:\n",
    "        valid = jnp.ones(tokens.shape[:2])\n",
    "    valid = valid.astype(jnp.float32)\n",
    "    sign = sign.astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "    token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(logits, axis=-1),\n",
    "            jnp.expand_dims(tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    sign_token_log_prob = jnp.where(valid > 0.0, token_log_prob * sign, jnp.array(0.0))\n",
    "    # sign_seq_log_prob = jnp.sum(sign_token_log_prob, axis=-1) /valid_text_length\n",
    "    # # loss = -jax.nn.log_sigmoid(sign_seq_log_prob).mean()\n",
    "    # loss = -sign_seq_log_prob.mean()\n",
    "\n",
    "    loss = -jax.nn.log_sigmoid(sign_token_log_prob.sum(axis=-1) / valid_text_length).mean()\n",
    "\n",
    "    # old: loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n",
    "    # changed to match hf implementation\n",
    "    correct = jnp.where(\n",
    "        (valid > 0.0) & (sign > 0.0),\n",
    "        jnp.argmax(logits, axis=-1) == tokens,\n",
    "        jnp.array(False)\n",
    "    )\n",
    "    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n",
    "    return loss, accuracy\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0, 0))\n",
    "def train_step_onpolicy(state: TrainState, input_tokens: jnp.ndarray, target_tokens: jnp.ndarray, sign, dropout_key) -> Tuple[jnp.ndarray, TrainState]:\n",
    "    dropout_key = jax.random.fold_in(dropout_key, state.step)\n",
    "    sign = sign.reshape(-1)[:, None].repeat(input_tokens.shape[-1], 1)\n",
    "    def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
    "        logits = state.apply_fn(params, input_tokens, False, rngs={'dropout': dropout_key})\n",
    "        loss, acc = signed_log_sigmoid_loss_and_accuracy(\n",
    "            logits, target_tokens, sign, (target_tokens > 0).astype(jnp.int32))\n",
    "        return loss, acc\n",
    "\n",
    "    # per-device loss and grads\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, acc), grads = grad_fn(state.params)\n",
    "    # average gradients across devices\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
    "    loss = jax.lax.pmean(loss, axis_name=\"batch\")\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return loss, acc, new_state\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "on_policy_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(np.array(original_dataset)), torch.tensor(np.array(generated_dataset)), torch.tensor(np.array(scores_dataset)))\n",
    "\n",
    "on_policy_loader = DataLoader(on_policy_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "on_policy_iter = iter(on_policy_loader) \n",
    "\n",
    "pbar = tqdm(on_policy_loader, total=len(on_policy_loader), desc='training on-policy')\n",
    "\n",
    "train_loss, train_acc = AverageMeter(), AverageMeter()\n",
    "\n",
    "val_loss, val_acc = jnp.inf, 0.\n",
    "\n",
    "step = 0.\n",
    "\n",
    "for original, generation, _ in pbar:\n",
    "\n",
    "    input_tokens = original[:, :-1].clone()\n",
    "    target_tokens = original.clone()\n",
    "    pred_tokens = generation.clone()\n",
    "    target_tokens[:, :train_data.num_prefix_tokens] = -1\n",
    "    pred_tokens[:, :train_data.num_prefix_tokens] = -1\n",
    "    target_tokens = target_tokens[:, 1:]\n",
    "    pred_tokens = pred_tokens[:, 1:]\n",
    "    sign = 2 * (target_tokens[:, -1] == pred_tokens[:, -1]).float() - 1.\n",
    "\n",
    "    input_tokens = torch.cat([input_tokens, input_tokens], axis=0)\n",
    "    target_tokens = torch.cat([target_tokens, pred_tokens], axis=0)    \n",
    "    sign = torch.cat([torch.ones(original.shape[0]), sign], axis=0)\n",
    "    \n",
    "    input_tokens = jnp.array(input_tokens)\n",
    "    target_tokens = jnp.array(target_tokens)\n",
    "    sign = jnp.array(sign)\n",
    "\n",
    "    input_tokens = input_tokens.reshape(jax.device_count(), -1, input_tokens.shape[-1])\n",
    "    target_tokens = target_tokens.reshape(jax.device_count(), -1, target_tokens.shape[-1])\n",
    "    sign = sign.reshape(jax.device_count(), -1)\n",
    "    \n",
    "    loss, acc, train_state = train_step_onpolicy(train_state, input_tokens, target_tokens, sign, keys_dropout)\n",
    "    train_loss.update(loss.mean(), input_tokens.shape[1] * jax.device_count())  \n",
    "    train_acc.update(acc.mean(), input_tokens.shape[1] * jax.device_count())    \n",
    "    if step % 10 == 0:\n",
    "        pbar.set_description(f'train loss: {train_loss.get()} train acc: {train_acc.get(percentage=True)} val loss: {val_loss} val acc: {val_acc}')\n",
    "    if step % 50 == 0:\n",
    "        val_loss, val_acc = evaluate(train_state, test_loader)\n",
    "        train_loss, train_acc = AverageMeter(), AverageMeter()\n",
    "    \n",
    "    step += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training on-policy:   0%|          | 0/1240 [03:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 25.00M. That was not possible. There are 21.28M free.; (0x0x0_HBM0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m target_tokens \u001b[38;5;241m=\u001b[39m target_tokens\u001b[38;5;241m.\u001b[39mreshape(jax\u001b[38;5;241m.\u001b[39mdevice_count(), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, target_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     37\u001b[0m sign \u001b[38;5;241m=\u001b[39m sign\u001b[38;5;241m.\u001b[39mreshape(jax\u001b[38;5;241m.\u001b[39mdevice_count(), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m loss, acc, train_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_onpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_dropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mmean(), input_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_count())  \n\u001b[1;32m     41\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mupdate(acc\u001b[38;5;241m.\u001b[39mmean(), input_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_count())    \n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:186\u001b[0m, in \u001b[0;36mbatched_device_put\u001b[0;34m(aval, sharding, xs, devices, committed)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bufs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(xs):\n\u001b[1;32m    184\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mArrayImpl(\n\u001b[1;32m    185\u001b[0m       aval, sharding, bufs, committed\u001b[38;5;241m=\u001b[39mcommitted, _skip_checks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatched_device_put\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommitted\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 25.00M. That was not possible. There are 21.28M free.; (0x0x0_HBM0)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = jax.device_get(train_state)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradientTransformationExtraArgs' object has no attribute 'hyperparams'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparams\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientTransformationExtraArgs' object has no attribute 'hyperparams'"
     ]
    }
   ],
   "source": [
    "train_state.tx.hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FrozenInstanceError",
     "evalue": "cannot assign to field 'tx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFrozenInstanceError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtx\u001b[49m \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mchain(        \n\u001b[1;32m      2\u001b[0m         optax\u001b[38;5;241m.\u001b[39mclip_by_global_norm(config\u001b[38;5;241m.\u001b[39mgrad_clip),\n\u001b[1;32m      3\u001b[0m         optax\u001b[38;5;241m.\u001b[39madamw(\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbetas, weight_decay\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mweight_decay, mask\u001b[38;5;241m=\u001b[39mparam_decay_mask(train_state\u001b[38;5;241m.\u001b[39mparams)),\n\u001b[1;32m      4\u001b[0m         optax\u001b[38;5;241m.\u001b[39mapply_every(config\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps),\n\u001b[1;32m      5\u001b[0m     )\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m: cannot assign to field 'tx'"
     ]
    }
   ],
   "source": [
    "train_state.tx = optax.chain(        \n",
    "        optax.clip_by_global_norm(config.grad_clip),\n",
    "        optax.adamw(1e-6, *config.betas, weight_decay=config.weight_decay, mask=param_decay_mask(train_state.params)),\n",
    "        optax.apply_every(config.gradient_accumulation_steps),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = optax.chain(\n",
    "    # Apply weight decay only to non-bias parameters\n",
    "    optax.clip_by_global_norm(config.grad_clip),\n",
    "    optax.adamw(learning_rate, *config.betas, weight_decay=config.weight_decay),\n",
    "    optax.apply_every(config.gradient_accumulation_steps),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "B.update.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
