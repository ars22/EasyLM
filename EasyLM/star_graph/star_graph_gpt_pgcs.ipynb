{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import time\n",
    "from jax_smi import initialise_tracking\n",
    "initialise_tracking()\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asetlur/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainConfig(gpt2_model_type='gpt2', seed=555, out_dir='out', shuffle_buffer_size=128, eval_interval=500, eval_steps=16, eval_only=False, keep_checkpoints=3, batch_size=128, train_steps=30, weight_decay=0.0, grad_clip=1.0, gradient_accumulation_steps=1, betas=(0.9, 0.95), learning_rate=StaticLRConfig(init_value=0.0001), wandb=WandbConfig(entity='ars22', project='star_graph', name='gpt2', mode='online', notes=''), model=GPTConfig(block_size=1024, vocab_size=50257, num_layers=12, num_heads=12, num_embeds=768, dropout_rate=0.1, use_bias=True, dtype=None), remat=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Tuple, Optional, Union\n",
    "from EasyLM.models.gpt2.gpt2_model import GPT, GPTConfig, get_pretrained_params\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class WandbConfig:\n",
    "    \"\"\"\n",
    "    wandb logging configuration\n",
    "    \"\"\"\n",
    "    entity: str = 'ars22'\n",
    "    \"\"\"username or team name where you're sending runs\"\"\"\n",
    "    project: str = 'star_graph'\n",
    "    \"\"\"project name\"\"\"\n",
    "    name: str = 'gpt2'\n",
    "    \"\"\"experiment name\"\"\"\n",
    "    mode: str = 'online'\n",
    "    \"\"\"'offline', 'online', or 'disabled'\"\"\"\n",
    "    notes: str = ''\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CosineDecayScheduleConfig:\n",
    "    init_value: float = 0.0\n",
    "    peak_value: float = 2.5e-4\n",
    "    warmup_steps: int = 2000\n",
    "    decay_steps: int = 150000\n",
    "    end_value: float = 1e-5\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StaticLRConfig:\n",
    "    init_value: float = 1e-4\n",
    "\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class TrainConfig:\n",
    "    gpt2_model_type: str = 'gpt2' # gpt2 model type\n",
    "    seed: int = 555\n",
    "    out_dir: str = 'out'                        # output directory for checkpoints (can be gcs path)\n",
    "    shuffle_buffer_size: int = 128\n",
    "    eval_interval: int = 500\n",
    "    eval_steps: int = 16        # evaluate for this number of steps (per-device)\n",
    "    eval_only: bool = False     # if True, script exits right after the first eval\n",
    "    keep_checkpoints: int = 3   # number of historical checkpoints to keep\n",
    "    batch_size: int = 128        # per-device batch size\n",
    "    train_steps: int = 30     # total number of training iterations\n",
    "    weight_decay: float = 0.0  # not applied to bias and embedding parameters\n",
    "    grad_clip: float = 1.0      # gradient norm clipping magnitude\n",
    "    gradient_accumulation_steps: int = 1    # used to simulate larger batch sizes\n",
    "    betas: Tuple[float, float] = (0.9, 0.95) # adamw optimizer betas\n",
    "    # learning_rate: CosineDecayScheduleConfig = field(default_factory=CosineDecayScheduleConfig)\n",
    "    learning_rate: StaticLRConfig = field(default_factory=StaticLRConfig)\n",
    "    wandb: WandbConfig = field(default_factory=WandbConfig) # wandb logging\n",
    "    model: GPTConfig = field(default_factory=GPTConfig)     # gpt model config\n",
    "    remat: bool = False    # set to True to rematerialize gradients during backward pass\n",
    "\n",
    "\n",
    "def get_default_config() -> TrainConfig:\n",
    "    return TrainConfig()\n",
    "\n",
    "config = get_default_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 05:05:59.156712: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2024-05-01 05:05:59.801494: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2024-05-01 05:05:59.801572: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2024-05-01 05:05:59.801578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax.core import FrozenDict, frozen_dict\n",
    "from flax.training import checkpoints\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "def prefix_target_list(filename=None):\n",
    "    \"\"\"\n",
    "    Load graphs and split them into prefix and target and return the list\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        prefix = line.strip().split('=')[0] + '='\n",
    "        target = line.strip().split('=')[1]\n",
    "        # target = target.split(',')[1]\n",
    "        data_list.append((prefix, target))\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class Graphs(Dataset):\n",
    "    def __init__(self, tokenizer, n_samples, data_path):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_samples = n_samples\n",
    "        self.data_path = data_path\n",
    "        self.eval_mode = False\n",
    "        self.data_file = prefix_target_list(self.data_path)\n",
    "        self.tokenized, self.num_prefix_tokens, self.num_target_tokens = self.tokenize(self.data_file[:n_samples])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.eval_mode:\n",
    "            # In eval mode return the entire sequence\n",
    "            return self.tokenized[idx].to(self.device)\n",
    "\n",
    "        # Create inputs\n",
    "        x = self.tokenized[idx].clone()\n",
    "        y = torch.cat([-torch.ones((self.num_prefix_tokens - 1, )),\n",
    "                       x[self.num_prefix_tokens:].clone()])\n",
    "        return x[:-1], y.long()\n",
    "\n",
    "    def tokenize(self, data_list):\n",
    "        \"\"\"\n",
    "        Takes a list of prefix-target pairs, tokenizes and concatenates them\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        prefix_len = len(self.tokenizer.encode(data_list[0][0]))\n",
    "        target_len = len(self.tokenizer.encode(data_list[0][1]))\n",
    "        same_len = True\n",
    "\n",
    "        for prefix, target in data_list:\n",
    "            prefix = torch.tensor(self.tokenizer.encode(prefix))\n",
    "            target = torch.tensor(self.tokenizer.encode(target))\n",
    "            if not (len(prefix) == prefix_len and len(target) == target_len):\n",
    "                same_len = False\n",
    "            seq = torch.concatenate([prefix, target], dim=-1).long()\n",
    "            out.append(seq)\n",
    "\n",
    "        # Check if all prefixes and all targets have the same length\n",
    "        if not same_len:\n",
    "            print('Not all prefixes or targets have the same length!!')\n",
    "        else:\n",
    "            print('Equal sequence lengths!')\n",
    "\n",
    "        return out, prefix_len, target_len\n",
    "\n",
    "    def eval(self):\n",
    "        # Switch to \"eval\" mode when generating sequences without teacher-forcing\n",
    "        self.eval_mode = True\n",
    "\n",
    "    def train(self):\n",
    "        # Switch back to \"train\" mode for teacher-forcing\n",
    "        self.eval_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "(tensor([21, 11, 24, 91, 22, 11, 21, 91, 24, 11, 15, 91, 20, 11, 17, 91, 22, 11,\n",
      "        19, 91, 19, 11, 20, 14, 22, 11, 17, 28, 22, 11, 19, 11, 20, 11]), tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 11, 19, 11, 20, 11, 17])) 6,9|7,6|9,0|5,2|7,4|4,5/7,2=7,4,5, 7,4,5,2\n"
     ]
    }
   ],
   "source": [
    "# LOAD TOKENIZER\n",
    "from transformers import AutoTokenizer # type: ignore\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.gpt2_model_type)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# LOAD DATASET\n",
    "data_path = 'deg_2_path_4_nodes_10'\n",
    "train_path, test_path = data_path + '_train_200000.txt', data_path + '_test_20000.txt'\n",
    "train_data = Graphs(tokenizer=tokenizer, n_samples=20000, data_path=train_path)\n",
    "test_data = Graphs(tokenizer=tokenizer, n_samples=500, data_path=test_path)\n",
    "train_data.train()\n",
    "\n",
    "# sanity check\n",
    "print(train_data[0], tokenizer.decode(train_data[0][0]), tokenizer.decode(train_data[0][1][-train_data.num_target_tokens:]))\n",
    "\n",
    "# LOAD DATALOADER\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, drop_last=True) \n",
    "test_loader = DataLoader(test_data, batch_size=config.batch_size, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer vocab size:  50257 28 7\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer vocab size: \", tokenizer.vocab_size, train_data.num_prefix_tokens, train_data.num_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_decay_mask(params: FrozenDict) -> FrozenDict:\n",
    "    \"\"\" pytree mask for non-bias parameters \"\"\"\n",
    "    flat_params = flax.traverse_util.flatten_dict(params)\n",
    "    flat_param_mask = {k: k[-1] not in ('bias', 'embedding', 'scale') for k in flat_params.keys()}\n",
    "    param_mask = flax.traverse_util.unflatten_dict(flat_param_mask)\n",
    "    return frozen_dict.freeze(param_mask)\n",
    "\n",
    "def init_train_state(key, config: TrainConfig, learning_rate) -> TrainState:\n",
    "\n",
    "    if config.remat:\n",
    "        model = flax.linen.remat(GPT,\n",
    "            static_argnums=(2,),\n",
    "            policy=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims)(config.model)\n",
    "    else:\n",
    "        config.model, params = get_pretrained_params(config.gpt2_model_type)\n",
    "        model = GPT(config.model)    \n",
    "        model.init(key)\n",
    "\n",
    "    optimizer = optax.chain(\n",
    "        # Apply weight decay only to non-bias parameters\n",
    "        optax.clip_by_global_norm(config.grad_clip),\n",
    "        optax.adamw(learning_rate, *config.betas, weight_decay=config.weight_decay, mask=param_decay_mask(params)),\n",
    "        optax.apply_every(config.gradient_accumulation_steps),\n",
    "    )\n",
    "\n",
    "    train_state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer)\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def count_params(params: FrozenDict) -> int:\n",
    "    p = jax.tree_util.tree_map(lambda a: a.size if isinstance(a, jnp.ndarray) else 0, params)\n",
    "    return jax.tree_util.tree_reduce(lambda a, b: a + b, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====  init parameters ============\n",
    "key = jax.random.PRNGKey(config.seed)\n",
    "key, key_params, key_dropout, key_generation = jax.random.split(key, 4)\n",
    "# make sure dropout keys are different for each device (local and global)\n",
    "key_dropout = jax.random.fold_in(key_dropout, jax.process_index())\n",
    "keys_dropout = jax.random.split(key_dropout, jax.local_device_count())\n",
    "key_gen = jax.random.split(key_generation, jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "source": [
    "learning_rate = config.learning_rate.init_value\n",
    "train_state = init_train_state(key_params, config, learning_rate)\n",
    "num_params = count_params(train_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters: {num_params:,}\") # 774,030,080 for gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict, freeze, unfreeze\n",
    "from transformers import FlaxGPT2LMHeadModel\n",
    "hf_model = FlaxGPT2LMHeadModel.from_pretrained(config.gpt2_model_type)\n",
    "hf_params = hf_model.init_weights(key_params, (2, config.model.block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicate model\n",
    "train_state = replicate(train_state)\n",
    "hf_params = replicate(hf_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_and_accuracy(logits, tokens, valid=None):\n",
    "    if valid is None:\n",
    "        valid = jnp.ones(tokens.shape[:2])\n",
    "    valid = valid.astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "    token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(logits, axis=-1),\n",
    "            jnp.expand_dims(tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n",
    "    loss = -(jnp.sum(token_log_prob) / jnp.sum(valid))\n",
    "    # old: loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n",
    "    # changed to match hf implementation\n",
    "    correct = jnp.where(\n",
    "        valid > 0.0,\n",
    "        jnp.argmax(logits, axis=-1) == tokens,\n",
    "        jnp.array(False)\n",
    "    )\n",
    "    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0))\n",
    "def train_step(state: TrainState, input_tokens: jnp.ndarray, target_tokens: jnp.ndarray, dropout_key):\n",
    "    dropout_key = jax.random.fold_in(dropout_key, state.step)\n",
    "    def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
    "        logits = state.apply_fn(params, input_tokens, False, rngs={'dropout': dropout_key})\n",
    "        \n",
    "        logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "        token_log_prob = jnp.squeeze(\n",
    "            jnp.take_along_axis(\n",
    "                jax.nn.log_softmax(logits, axis=-1),\n",
    "                jnp.expand_dims(target_tokens, -1),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        prob_hard_token = jnp.exp(token_log_prob[:, train_data.num_prefix_tokens+1]).mean()\n",
    "        \n",
    "        loss, acc = cross_entropy_loss_and_accuracy(\n",
    "            logits, target_tokens, (target_tokens > 0).astype(jnp.int32))\n",
    "        \n",
    "        return loss, (prob_hard_token, acc)\n",
    "    # per-device loss and grads\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (prob_hard_token, acc)), grads = grad_fn(state.params)\n",
    "    # average gradients across devices\n",
    "    prob_hard_token = jax.lax.pmean(prob_hard_token, axis_name=\"batch\")\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
    "    loss = jax.lax.pmean(loss, axis_name=\"batch\")\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    return loss, acc, prob_hard_token, new_state\n",
    "\n",
    "\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "def convert_jax_params_to_hf(hf_params, jax_params) -> FrozenDict:\n",
    "    hf_params = unfreeze(hf_params)\n",
    "    \n",
    "    for k in ['ln_f', 'wpe', 'wte']:\n",
    "        hf_params['transformer'][k] = jax_params[k]\n",
    "    for k in hf_params['transformer']['h'].keys():\n",
    "        hf_params['transformer']['h'][k] = jax_params[k] \n",
    "\n",
    "    hf_params = flatten_dict(hf_params, sep='.')\n",
    "    for k in hf_params.keys():\n",
    "        if k.endswith('attn.c_attn.kernel'):\n",
    "            hf_params[k] = hf_params[k].T\n",
    "        elif k.endswith('attn.c_proj.kernel'):\n",
    "            hf_params[k] = hf_params[k].T\n",
    "        elif len(k.split('.')) > 3 and k.split('.')[3] == 'mlp' and k.endswith('kernel'):\n",
    "            hf_params[k] = hf_params[k].T\n",
    "    hf_params = unflatten_dict({k: v for k, v in hf_params.items()}, sep='.')\n",
    "    return freeze(hf_params)\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0))\n",
    "def eval_step(hf_params, state, input_tokens: jnp.ndarray, target_tokens: jnp.ndarray):\n",
    "    hf_params = convert_jax_params_to_hf(hf_params, state.params['params'])\n",
    "    output = hf_model.generate(\n",
    "        input_tokens[:, :train_data.num_prefix_tokens],\n",
    "        params=hf_params,\n",
    "        max_new_tokens=train_data.num_target_tokens,\n",
    "        min_length=train_data.num_target_tokens+train_data.num_prefix_tokens, \n",
    "        do_sample=False, \n",
    "        attention_mask=jnp.ones_like(input_tokens[:, :train_data.num_prefix_tokens]))\n",
    "    acc = ((output[0][:, -train_data.num_target_tokens:] == target_tokens[:, -train_data.num_target_tokens:]).sum(1) == train_data.num_target_tokens).mean()\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    return acc\n",
    "\n",
    "max_new_tokens = train_data.num_target_tokens\n",
    "num_beams=2\n",
    "num_return_sequences=2\n",
    "temperature=1.0\n",
    "\n",
    "from flax.core import FrozenDict, freeze, unfreeze\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0))\n",
    "def generate_negative_data(hf_params, train_state, input_tokens, key):\n",
    "    hf_params = convert_jax_params_to_hf(hf_params, train_state.params['params'])\n",
    "    return hf_model.generate(\n",
    "        input_tokens[:, :train_data.num_prefix_tokens],\n",
    "        params=hf_params,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        min_length=train_data.num_target_tokens+train_data.num_prefix_tokens,\n",
    "        prng_key=key, \n",
    "        num_beams=num_beams, \n",
    "        num_return_sequences=num_return_sequences, \n",
    "        temperature=1.0,\n",
    "        attention_mask=jnp.ones_like(input_tokens[:, :train_data.num_prefix_tokens]))\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(hf_params, state: TrainState, loader: DataLoader) -> jnp.ndarray:\n",
    "    accs = []\n",
    "    for batch in loader:\n",
    "        input_tokens, target_tokens = batch\n",
    "        input_tokens = jnp.array(input_tokens)\n",
    "        target_tokens = jnp.array(target_tokens)\n",
    "        input_tokens = input_tokens.reshape(jax.local_device_count(), -1, input_tokens.shape[-1])\n",
    "        target_tokens = target_tokens.reshape(jax.local_device_count(), -1, target_tokens.shape[-1])\n",
    "        acc = eval_step(hf_params, state, input_tokens, target_tokens)\n",
    "        accs.append(acc)\n",
    "    return jnp.mean(jnp.stack(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.val = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val += val * num\n",
    "        self.num += num\n",
    "\n",
    "    def get(self, percentage=False):\n",
    "        val = self.val / self.num * 100 if percentage else self.val / self.num\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(state, input_tokens, target_tokens, dropout_key):\n",
    "    logits = state.apply_fn(state.params, input_tokens, False, rngs={'dropout': dropout_key})\n",
    "    valid = (target_tokens > 0).astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    \n",
    "    logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "    token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(logits, axis=-1),\n",
    "            jnp.expand_dims(target_tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n",
    "    return token_log_prob \n",
    "\n",
    "def get_token_level_scores(original_dataset, generated_dataset):\n",
    "    token_scores_arr = []\n",
    "    for i in range(original_dataset.shape[0]):\n",
    "        original_seq = original_dataset[i]\n",
    "        generated_seq = generated_dataset[i]\n",
    "        token_scores = (original_seq == generated_seq).float()\n",
    "        incorrect = torch.where(token_scores==0)\n",
    "        if len(incorrect[0]) > 0:\n",
    "            token_scores[incorrect[0][0].item()] = -1. \n",
    "            token_scores[incorrect[0][0].item()+1:] = 0. \n",
    "        token_scores[:train_data.num_prefix_tokens] = 0.\n",
    "        token_scores_arr.append(token_scores)\n",
    "    return torch.stack(token_scores_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# loss =  (pos_token_log_prob - neg_token_log_prob).mean()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, accuracy\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jax\u001b[38;5;241m.\u001b[39mpmap, axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step_onpolicy\u001b[39m(state: TrainState, pos_input_tokens, pos_target_tokens, neg_input_tokens, neg_target_tokens, dropout_key) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[jnp\u001b[38;5;241m.\u001b[39mndarray, TrainState]:\n\u001b[1;32m     83\u001b[0m     dropout_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mfold_in(dropout_key, state\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params: FrozenDict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "# def signed_log_sigmoid_loss_and_accuracy(logits, tokens, scores, valid=None):\n",
    "#     if valid is None:\n",
    "#         valid = jnp.ones(tokens.shape[:2])\n",
    "#     valid = valid.astype(jnp.float32)\n",
    "#     scores = scores.astype(jnp.float32)\n",
    "#     valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "#     logits = logits.astype(jnp.float32)  # for numerical stability\n",
    "#     token_log_prob = jnp.squeeze(\n",
    "#         jnp.take_along_axis(\n",
    "#             jax.nn.log_softmax(logits, axis=-1),\n",
    "#             jnp.expand_dims(tokens, -1),\n",
    "#             axis=-1,\n",
    "#         ),\n",
    "#         -1,\n",
    "#     )\n",
    "#     sign_token_log_prob = jnp.where(valid > 0.0, token_log_prob * scores, jnp.array(0.0))\n",
    "#     # sign_seq_log_prob = jnp.sum(sign_token_log_prob, axis=-1) /valid_text_length\n",
    "#     # # loss = -jax.nn.log_sigmoid(sign_seq_log_prob).mean()\n",
    "#     # loss = -sign_seq_log_prob.mean()\n",
    "\n",
    "#     # loss = -(sign_token_log_prob.sum(axis=-1)).mean()\n",
    "#     loss = -jax.nn.log_sigmoid(sign_token_log_prob.sum(axis=-1) / valid_text_length).mean()\n",
    "\n",
    "#     # old: loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n",
    "#     # changed to match hf implementation\n",
    "#     correct = jnp.where(\n",
    "#         (valid > 0.0) & (scores > 0.0),\n",
    "#         jnp.argmax(logits, axis=-1) == tokens,\n",
    "#         jnp.array(False)\n",
    "#     )\n",
    "#     accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n",
    "#     return loss, accuracy\n",
    "\n",
    "\n",
    "def dpo_loss(pos_logits, pos_tokens, neg_logits, neg_tokens):\n",
    "    valid = (pos_tokens > 0).astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    pos_logits = pos_logits.astype(jnp.float32)  # for numerical stability\n",
    "    pos_token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(pos_logits, axis=-1),\n",
    "            jnp.expand_dims(pos_tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    pos_token_log_prob = jnp.where(valid > 0.0, pos_token_log_prob, jnp.array(0.0))\n",
    "    pos_token_log_prob = pos_token_log_prob.sum(axis=-1) \n",
    "    # / valid_text_length\n",
    "\n",
    "    correct = jnp.where(\n",
    "        (valid > 0.0),\n",
    "        jnp.argmax(pos_logits, axis=-1) == pos_tokens,\n",
    "        jnp.array(False)\n",
    "    )\n",
    "    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n",
    "    \n",
    "\n",
    "    valid = (neg_tokens > 0).astype(jnp.float32)\n",
    "    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    neg_logits = neg_logits.astype(jnp.float32)  # for numerical stability\n",
    "    neg_token_log_prob = jnp.squeeze(\n",
    "        jnp.take_along_axis(\n",
    "            jax.nn.log_softmax(neg_logits, axis=-1),\n",
    "            jnp.expand_dims(neg_tokens, -1),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    neg_token_log_prob = jnp.where(valid > 0.0, neg_token_log_prob, jnp.array(0.0))\n",
    "    neg_token_log_prob = neg_token_log_prob.sum(axis=-1)\n",
    "    # / valid_text_length\n",
    "\n",
    "    loss =  (-jax.nn.log_sigmoid(1. * (pos_token_log_prob - neg_token_log_prob))).mean()\n",
    "    # loss =  (pos_token_log_prob - neg_token_log_prob).mean()\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, 0, 0, 0))\n",
    "def train_step_onpolicy(state: TrainState, pos_input_tokens, pos_target_tokens, neg_input_tokens, neg_target_tokens, dropout_key) -> Tuple[jnp.ndarray, TrainState]:\n",
    "    dropout_key = jax.random.fold_in(dropout_key, state.step)\n",
    "    def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
    "        pos_logits = state.apply_fn(params, pos_input_tokens, False, rngs={'dropout': dropout_key})\n",
    "        neg_logits = state.apply_fn(params, neg_input_tokens, False, rngs={'dropout': dropout_key})\n",
    "        loss, acc = dpo_loss(\n",
    "            pos_logits, pos_target_tokens, neg_logits, neg_target_tokens)\n",
    "        token_log_prob = jnp.squeeze(\n",
    "            jnp.take_along_axis(\n",
    "                jax.nn.log_softmax(pos_logits, axis=-1),\n",
    "                jnp.expand_dims(pos_target_tokens, -1),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        prob_hard_token = jnp.exp(token_log_prob[:, train_data.num_prefix_tokens+1]).mean()\n",
    "        return loss, (prob_hard_token, acc)    \n",
    "        \n",
    "    # per-device loss and grads\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (prob_hard_token, acc)), grads = grad_fn(state.params)\n",
    "    # average gradients across devices\n",
    "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
    "    loss = jax.lax.pmean(loss, axis_name=\"batch\")\n",
    "    acc = jax.lax.pmean(acc, axis_name=\"batch\")\n",
    "    prob_hard_token = jax.lax.pmean(prob_hard_token, axis_name=\"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return loss, acc, prob_hard_token, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# original_dataset = []\n",
    "# generated_dataset = []\n",
    "# # scores_dataset = []\n",
    "# loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True) \n",
    "\n",
    "# for input_tokens, target_tokens in tqdm(iter(loader)): \n",
    "#     input_tokens = jnp.array(input_tokens)\n",
    "#     target_tokens = jnp.array(target_tokens)\n",
    "#     original = jnp.concatenate([input_tokens[:, :train_data.num_prefix_tokens], target_tokens[:, -train_data.num_target_tokens:]], axis=1)\n",
    "    \n",
    "#     input_tokens = input_tokens.reshape(jax.device_count(), -1, input_tokens.shape[-1])\n",
    "\n",
    "#     generations = generate_negative_data(hf_params, train_state, input_tokens, key_gen)\n",
    "    \n",
    "#     repeated_original = jnp.repeat(original[None, :, :], num_return_sequences, 0).transpose(1, 0, 2)\n",
    "    \n",
    "#     original_dataset.append(repeated_original.reshape(-1, repeated_original.shape[-1]))\n",
    "#     generated_dataset.append(generations[0].reshape(-1, generations[0].shape[-1]))\n",
    "    \n",
    "#     # scores_dataset.append(generations[1].reshape(-1, generations[1].shape[-1]))\n",
    "\n",
    "# # scores_dataset = jnp.concatenate([x.reshape(-1) for x in scores_dataset], axis=0).squeeze()\n",
    "# generated_dataset = jnp.concatenate(generated_dataset, axis=0)\n",
    "# original_dataset = jnp.concatenate(original_dataset, axis=0)\n",
    "# token_lvl_scores = get_token_level_scores( torch.tensor(np.array(original_dataset)), torch.tensor(np.array(generated_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_lvl_scores.shape, len(train_data) // 64 * 64 * 5, len(generated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_policy_dataset = torch.utils.data.TensorDataset(\n",
    "#     torch.tensor(np.array(original_dataset)), torch.tensor(np.array(generated_dataset)), token_lvl_scores)\n",
    "\n",
    "# on_policy_loader = DataLoader(on_policy_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "# on_policy_iter = iter(on_policy_loader) \n",
    "\n",
    "# pbar = tqdm(on_policy_loader, total=len(on_policy_loader), desc='training on-policy')\n",
    "\n",
    "# train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter() \n",
    "# policy_train_acc, policy_test_acc = 0., 0.\n",
    "\n",
    "# step = 0.\n",
    "\n",
    "# for original, generation, scores in pbar:\n",
    "\n",
    "#     pos_input_tokens = original.clone()\n",
    "#     pos_input_tokens = pos_input_tokens[:, :-1]\n",
    "#     pos_target_tokens = original.clone()\n",
    "#     pos_target_tokens[:, :train_data.num_prefix_tokens] = -1\n",
    "#     pos_target_tokens = pos_target_tokens[:, 1:]\n",
    "\n",
    "#     neg_input_tokens = generation.clone()\n",
    "#     neg_input_tokens = neg_input_tokens[:, :-1]\n",
    "#     neg_target_tokens = generation.clone()\n",
    "#     neg_target_tokens[scores == 0] = -1\n",
    "#     neg_target_tokens = neg_target_tokens[:, 1:]\n",
    "\n",
    "    \n",
    "#     pos_input_tokens = jnp.array(pos_input_tokens)\n",
    "#     neg_input_tokens = jnp.array(neg_input_tokens)\n",
    "#     pos_target_tokens = jnp.array(pos_target_tokens)\n",
    "#     neg_target_tokens = jnp.array(neg_target_tokens)\n",
    "\n",
    "#     pos_input_tokens = pos_input_tokens.reshape(jax.device_count(), -1, pos_input_tokens.shape[-1])\n",
    "#     pos_target_tokens = pos_target_tokens.reshape(jax.device_count(), -1, pos_target_tokens.shape[-1])\n",
    "#     neg_input_tokens = neg_input_tokens.reshape(jax.device_count(), -1, neg_input_tokens.shape[-1])\n",
    "#     neg_target_tokens = neg_target_tokens.reshape(jax.device_count(), -1, neg_target_tokens.shape[-1])\n",
    "\n",
    "    \n",
    "#     # print(pos_input_tokens.shape, pos_target_tokens.shape, neg_input_tokens.shape, neg_target_tokens.shape) \n",
    "#     # break\n",
    "\n",
    "#     loss, acc, train_state = train_step_onpolicy(train_state, pos_input_tokens, pos_target_tokens, neg_input_tokens, neg_target_tokens, keys_dropout)\n",
    "#     train_loss.update(loss.mean(), pos_input_tokens.shape[1] * jax.device_count())  \n",
    "#     train_acc.update(acc.mean(), pos_input_tokens.shape[1] * jax.device_count())    \n",
    "\n",
    "#     # phard.update(phard_token.mean(), input_tokens.shape[1] * jax.device_count())\n",
    "#     if step % 10 == 0:\n",
    "#         pbar.set_description(f'train loss: {train_loss.get()} forcing train acc: {train_acc.get(percentage=True)} policy train acc: {100. * policy_train_acc} policy test acc: {100. * policy_test_acc}')\n",
    "#     if step % config.eval_interval == 0:\n",
    "#         policy_train_acc = evaluate(hf_params, train_state, train_loader)\n",
    "#         policy_test_acc = evaluate(hf_params, train_state, test_loader)\n",
    "#         train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    \n",
    "#     step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training on-policy:   0%|          | 0/156 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/asetlur/.local/lib/python3.8/site-packages/jax/_src/interpreters/mlir.py:246: RuntimeWarning: overflow encountered in cast\n",
      "  x = np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n",
      "train loss: 0.6976994872093201 forcing train acc: 9.440103530883789 policy train acc: 0.0 policy test acc: 0.0:   0%|          | 0/156 [02:23<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "train loss: 0.6934075355529785 forcing train acc: 77.08564758300781 policy train acc: 0.4457131624221802 policy test acc: 0.2604166865348816: 100%|██████████| 156/156 [03:12<00:00,  1.23s/it]\n",
      "train loss: 0.6931999325752258 forcing train acc: 78.8960189819336 policy train acc: 42.76342010498047 policy test acc: 42.96875: 100%|██████████| 156/156 [00:22<00:00,  6.85it/s] \n",
      "train loss: 0.6931730508804321 forcing train acc: 81.21218872070312 policy train acc: 51.09174728393555 policy test acc: 48.69791793823242: 100%|██████████| 156/156 [00:22<00:00,  6.82it/s]\n",
      "train loss: 0.6932268142700195 forcing train acc: 81.67512512207031 policy train acc: 51.21194076538086 policy test acc: 51.5625: 100%|██████████| 156/156 [00:22<00:00,  6.89it/s]\n",
      "train loss: 0.6931969523429871 forcing train acc: 83.65673065185547 policy train acc: 50.7912712097168 policy test acc: 44.79166793823242: 100%|██████████| 156/156 [00:22<00:00,  6.89it/s]\n",
      "train loss: 0.6931537985801697 forcing train acc: 83.3914794921875 policy train acc: 51.652645111083984 policy test acc: 52.083335876464844: 100%|██████████| 156/156 [00:22<00:00,  6.90it/s] \n",
      "train loss: 0.6931578516960144 forcing train acc: 84.8822021484375 policy train acc: 51.7628173828125 policy test acc: 49.21875: 100%|██████████| 156/156 [00:23<00:00,  6.76it/s] \n",
      "train loss: 0.6930265426635742 forcing train acc: 85.78116607666016 policy train acc: 52.78946304321289 policy test acc: 51.04166793823242: 100%|██████████| 156/156 [00:23<00:00,  6.66it/s]\n",
      "train loss: 0.6929879188537598 forcing train acc: 85.91683959960938 policy train acc: 54.06650924682617 policy test acc: 52.083335876464844: 100%|██████████| 156/156 [00:23<00:00,  6.70it/s]\n",
      "train loss: 0.6928454637527466 forcing train acc: 86.50379943847656 policy train acc: 53.58073043823242 policy test acc: 56.770835876464844: 100%|██████████| 156/156 [00:23<00:00,  6.62it/s]\n",
      "train loss: 0.6926096081733704 forcing train acc: 86.9458236694336 policy train acc: 54.37700271606445 policy test acc: 50.520835876464844: 100%|██████████| 156/156 [00:23<00:00,  6.75it/s] \n",
      "train loss: 0.6923183798789978 forcing train acc: 87.53804016113281 policy train acc: 54.1165885925293 policy test acc: 46.35416793823242: 100%|██████████| 156/156 [00:22<00:00,  6.85it/s]\n",
      "train loss: 0.691604733467102 forcing train acc: 87.87921905517578 policy train acc: 56.03966522216797 policy test acc: 51.04166793823242: 100%|██████████| 156/156 [00:22<00:00,  6.91it/s] \n",
      "train loss: 0.6904565691947937 forcing train acc: 87.5323715209961 policy train acc: 56.4503173828125 policy test acc: 53.90625:   8%|▊         | 13/156 [00:03<00:42,  3.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# print(pos_input_tokens.shape, pos_target_tokens.shape, neg_input_tokens.shape, neg_target_tokens.shape) \u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m     54\u001b[0m loss, acc, train_state \u001b[38;5;241m=\u001b[39m train_step_onpolicy(train_state, pos_input_tokens, pos_target_tokens, neg_input_tokens, neg_target_tokens, keys_dropout)\n\u001b[0;32m---> 55\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, pos_input_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_count())  \n\u001b[1;32m     56\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mupdate(acc\u001b[38;5;241m.\u001b[39mmean(), pos_input_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_count())    \n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# phard.update(phard_token.mean(), input_tokens.shape[1] * jax.device_count())\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/reductions.py:316\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;129m@_wraps\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean, skip_params\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(a: ArrayLike, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m          out: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, keepdims: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    315\u001b[0m          where: Optional[ArrayLike] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m--> 316\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ensure_optional_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m               \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:250\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 250\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_params_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    253\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:163\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   dispatch\u001b[38;5;241m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:2677\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2674\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2675\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2676\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1203\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1200\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m   1201\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1202\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1187\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1187\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1193\u001b[0m       compiled, tree_structure(out_flat), args, out_flat)\n\u001b[1;32m   1194\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1143\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   distributed_debug_log((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning pjit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md function\u001b[39m\u001b[38;5;124m\"\u001b[39m, name),\n\u001b[1;32m   1138\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_shardings\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_shardings),\n\u001b[1;32m   1139\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_shardings\u001b[39m\u001b[38;5;124m\"\u001b[39m, out_shardings),\n\u001b[1;32m   1140\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract args\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmap\u001b[39m(xla\u001b[38;5;241m.\u001b[39mabstractify, args)),\n\u001b[1;32m   1141\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfingerprint\u001b[39m\u001b[38;5;124m\"\u001b[39m, fingerprint))\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1143\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsafe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, compiled\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m:\n\u001b[1;32m   1145\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_debug_nans \u001b[38;5;129;01mor\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_debug_infs  \u001b[38;5;66;03m# compiled_fun can only raise in this case\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:1337\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;129m@profiler\u001b[39m\u001b[38;5;241m.\u001b[39mannotate_function\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1336\u001b[0m   args \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkept_var_idx]\n\u001b[0;32m-> 1337\u001b[0m   input_bufs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1338\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_unordered_effects\n\u001b[1;32m   1339\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_host_callbacks):\n\u001b[1;32m   1340\u001b[0m     input_bufs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_tokens_to_inputs(input_bufs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:1181\u001b[0m, in \u001b[0;36mInputsHandler.__call__\u001b[0;34m(self, input_buffers)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_buffers):\n\u001b[0;32m-> 1181\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:140\u001b[0m, in \u001b[0;36mshard_args\u001b[0;34m(devices, indices, shardings, args)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@profiler\u001b[39m\u001b[38;5;241m.\u001b[39mannotate_function\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshard_args\u001b[39m(\n\u001b[1;32m    121\u001b[0m     devices: Sequence[xb\u001b[38;5;241m.\u001b[39mxla_client\u001b[38;5;241m.\u001b[39mDevice],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     args,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[jax\u001b[38;5;241m.\u001b[39mArray]:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Shard each argument data array along its leading axis.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    for each argument.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [shard_arg(arg, devices, indices[i], shardings[i])\n\u001b[1;32m    141\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args)]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:140\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@profiler\u001b[39m\u001b[38;5;241m.\u001b[39mannotate_function\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshard_args\u001b[39m(\n\u001b[1;32m    121\u001b[0m     devices: Sequence[xb\u001b[38;5;241m.\u001b[39mxla_client\u001b[38;5;241m.\u001b[39mDevice],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     args,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[jax\u001b[38;5;241m.\u001b[39mArray]:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Shard each argument data array along its leading axis.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    for each argument.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mshard_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshardings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args)]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:116\u001b[0m, in \u001b[0;36mshard_arg\u001b[0;34m(arg, devices, arg_indices, sharding)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of size len(devices) containing per-device buffers.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mFor the C++ pmap path, we fallback to Python (this function) to shard\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m  arg_indices: A list of `len(devices)` indices to use to shard the argument.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m arg \u001b[38;5;241m=\u001b[39m xla\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(arg)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshard_arg_handlers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/array.py:702\u001b[0m, in \u001b[0;36m_array_shard_arg\u001b[0;34m(x, devices, indices, sharding)\u001b[0m\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mshard_device_array(x, devices, indices, sharding)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 702\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_sharded_device_array_slow_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:370\u001b[0m, in \u001b[0;36mshard_sharded_device_array_slow_path\u001b[0;34m(x, devices, indices, sharding)\u001b[0m\n\u001b[1;32m    366\u001b[0m candidates_list \u001b[38;5;241m=\u001b[39m candidates[_hashable_index(idx)]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates_list:\n\u001b[1;32m    368\u001b[0m   \u001b[38;5;66;03m# This array isn't sharded correctly. Reshard it via host roundtrip.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m   \u001b[38;5;66;03m# TODO(skye): more efficient reshard?\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m shard_arg_handlers[\u001b[38;5;28mtype\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)](\n\u001b[1;32m    371\u001b[0m       x\u001b[38;5;241m.\u001b[39m_value, devices, indices, sharding)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Try to find a candidate buffer already on the correct device,\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# otherwise copy one of them.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m candidates_list:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/array.py:535\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m npy_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, arr \u001b[38;5;129;01min\u001b[39;00m copy_plan:\n\u001b[0;32m--> 535\u001b[0m   npy_value[ind] \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m npy_value  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for ep in range(50):\n",
    "    loader = DataLoader(train_data, batch_size=128, shuffle=True, drop_last=True) \n",
    "    train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter() \n",
    "    policy_train_acc, policy_test_acc = 0., 0.\n",
    "\n",
    "    step = 0\n",
    "    pbar = tqdm(loader, total=len(loader), desc='training on-policy')\n",
    "\n",
    "    for input_tokens, target_tokens in pbar: \n",
    "        input_tokens = jnp.array(input_tokens)\n",
    "        target_tokens = jnp.array(target_tokens)\n",
    "        original = jnp.concatenate([input_tokens[:, :train_data.num_prefix_tokens], target_tokens[:, -train_data.num_target_tokens:]], axis=1)\n",
    "        \n",
    "        input_tokens = input_tokens.reshape(jax.device_count(), -1, input_tokens.shape[-1])\n",
    "        generations = generate_negative_data(hf_params, train_state, input_tokens, key_gen)\n",
    "        generations = generations[0].reshape(-1, generations[0].shape[-1]) \n",
    "        repeated_original = jnp.repeat(original[None, :, :], num_return_sequences, 0).transpose(1, 0, 2)\n",
    "        original = repeated_original.reshape(-1, repeated_original.shape[-1])\n",
    "        original = torch.tensor(np.array(original))\n",
    "        generations = torch.tensor(np.array(generations))\n",
    "        token_lvl_scores = get_token_level_scores(original , generations)\n",
    "        \n",
    "\n",
    "        pos_input_tokens = original.clone()\n",
    "        pos_input_tokens = pos_input_tokens[:, :-1]\n",
    "        pos_target_tokens = original.clone()\n",
    "        # pos_target_tokens[:, :train_data.num_prefix_tokens] = -1\n",
    "        pos_target_tokens[token_lvl_scores == 0] = -1   \n",
    "        pos_target_tokens = pos_target_tokens[:, 1:]\n",
    "\n",
    "        neg_input_tokens = generations.clone()\n",
    "        neg_input_tokens = neg_input_tokens[:, :-1]\n",
    "        neg_target_tokens = generations.clone()\n",
    "        neg_target_tokens[token_lvl_scores == 0] = -1\n",
    "        neg_target_tokens = neg_target_tokens[:, 1:]\n",
    "\n",
    "        \n",
    "        pos_input_tokens = jnp.array(pos_input_tokens)\n",
    "        neg_input_tokens = jnp.array(neg_input_tokens)\n",
    "        pos_target_tokens = jnp.array(pos_target_tokens)\n",
    "        neg_target_tokens = jnp.array(neg_target_tokens)\n",
    "\n",
    "        pos_input_tokens = pos_input_tokens.reshape(jax.device_count(), -1, pos_input_tokens.shape[-1])\n",
    "        pos_target_tokens = pos_target_tokens.reshape(jax.device_count(), -1, pos_target_tokens.shape[-1])\n",
    "        neg_input_tokens = neg_input_tokens.reshape(jax.device_count(), -1, neg_input_tokens.shape[-1])\n",
    "        neg_target_tokens = neg_target_tokens.reshape(jax.device_count(), -1, neg_target_tokens.shape[-1])\n",
    "\n",
    "        \n",
    "        # print(pos_input_tokens.shape, pos_target_tokens.shape, neg_input_tokens.shape, neg_target_tokens.shape) \n",
    "        # break\n",
    "\n",
    "        loss, acc, prob_hard_token, train_state = train_step_onpolicy(train_state, pos_input_tokens, pos_target_tokens, neg_input_tokens, neg_target_tokens, keys_dropout)\n",
    "        train_loss.update(loss.mean(), pos_input_tokens.shape[1] * jax.device_count())  \n",
    "        train_acc.update(acc.mean(), pos_input_tokens.shape[1] * jax.device_count())   \n",
    "        phard.update(prob_hard_token.mean(), pos_input_tokens.shape[1] * jax.device_count()) \n",
    "\n",
    "        # phard.update(phard_token.mean(), input_tokens.shape[1] * jax.device_count())\n",
    "        if step % 10 == 0:\n",
    "            pbar.set_description(f'train loss: {train_loss.get()} phard: {phard.get()} forcing train acc: {train_acc.get(percentage=True)} policy train acc: {100. * policy_train_acc} policy test acc: {100. * policy_test_acc}')\n",
    "        if step % config.eval_interval == 0:\n",
    "            policy_train_acc = evaluate(hf_params, train_state, train_loader)\n",
    "            policy_test_acc = evaluate(hf_params, train_state, test_loader)\n",
    "            train_loss, train_acc, phard = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "        \n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 160, 35)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_negative_data(hf_params, train_state, input_tokens, key_gen)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([18, 11, 15, 91, 23, 11, 16, 91, 15, 11, 17, 91, 17, 11, 19, 91, 16,\n",
       "        11, 21, 91, 18, 11, 23, 14, 18, 11, 19, 28, 18, 11, 15, 11, 17, 11],      dtype=int32),\n",
       " Array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 18, 11, 15, -1, -1, -1, -1],      dtype=int32),\n",
       " Array([18, 11, 15, 91, 23, 11, 16, 91, 15, 11, 17, 91, 17, 11, 19, 91, 16,\n",
       "        11, 21, 91, 18, 11, 23, 14, 18, 11, 19, 28, 18, 11, 22, 11, 23, 11],      dtype=int32),\n",
       " Array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 18, 11, 22, -1, -1, -1, -1],      dtype=int32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 3\n",
    "pos_input_tokens[j][j], pos_target_tokens[j][j], neg_input_tokens[j][j], neg_target_tokens[j][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
